{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f8081cf2fe0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"/root/circuit-finder\")\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "import transformer_lens as tl\n",
    "from circuit_finder.patching.eap_graph import EAPGraph\n",
    "from circuit_finder.utils import clear_memory\n",
    "from circuit_finder.constants import device\n",
    "from circuit_finder.pretrained import (\n",
    "    load_attn_saes,\n",
    "    load_hooked_mlp_transcoders,\n",
    ")\n",
    "from circuit_finder.metrics import batch_avg_answer_diff\n",
    "from circuit_finder.patching.causal_interventions import run_with_ablations\n",
    "import os\n",
    "os.chdir(\"/root/circuit-finder/\")\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff08344807c045cd855a28ac5ba23897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/circuit-finder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize SAEs\n",
    "attn_saes = load_attn_saes(use_error_term=True)\n",
    "mlp_transcoders = load_hooked_mlp_transcoders(use_error_term=True)\n",
    "\n",
    "# Load model\n",
    "model = tl.HookedSAETransformer.from_pretrained(\"gpt2\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Prompts: ['Then, Jeremy and Scott went to the hospital. Jeremy gave a snack to', 'Then, Victoria and Stephen went to the station. Victoria gave a necklace to', 'Then, Carter and Kyle went to the restaurant. Carter gave a kiss to', 'Then, Robert and Lisa went to the restaurant. Robert gave a computer to', 'Then, Andy and Ruby went to the office. Andy gave a snack to', 'Then, John and Jacob went to the school. John gave a snack to', 'Then, Emily and Leon went to the store. Emily gave a kiss to', 'Then, Tyler and Robert went to the store. Tyler gave a snack to', 'Then, Ruby and Edward went to the garden. Ruby gave a basketball to', 'Then, Sullivan and Dean went to the station. Sullivan gave a kiss to']\n",
      "Corrupt Prompts: ['Then, Michael and Anderson went to the hospital. Rachel gave a snack to', 'Then, Charlie and Jay went to the station. Elizabeth gave a necklace to', 'Then, Emily and River went to the restaurant. Brian gave a kiss to', 'Then, Daniel and Austin went to the restaurant. Rose gave a computer to', 'Then, Brandon and Brian went to the office. James gave a snack to', 'Then, Anderson and Aaron went to the school. Charlie gave a snack to', 'Then, Frank and Clark went to the store. Sullivan gave a kiss to', 'Then, Matthew and Max went to the store. Alex gave a snack to', 'Then, Charles and Stephen went to the garden. Connor gave a basketball to', 'Then, Alex and Laura went to the station. Roman gave a kiss to']\n",
      "Correct IDs: tensor([ 4746,  7970, 14316, 15378, 10888, 12806, 10592,  5199, 10443, 11325],\n",
      "       device='cuda:0')\n",
      "Wrong IDs: tensor([11753, 12313, 10831,  5199, 12382,  1757, 17608, 14886, 10888, 18501],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def load_batch(file_path, batch_size=10, first_batch=0):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    word_idxs = data['word_idxs']\n",
    "\n",
    "    prompts = data['prompts'][first_batch:first_batch+batch_size]\n",
    "\n",
    "    clean_prompts = [prompt['clean'] for prompt in prompts]\n",
    "    corrupt_prompts = [prompt['corrupt'] for prompt in prompts]\n",
    "    correct_answers = [prompt['answers'][0] for prompt in prompts]\n",
    "    wrong_answers = [prompt['wrong_answers'][0] for prompt in prompts]\n",
    "   \n",
    "\n",
    "    correct_ids = model.to_tokens(correct_answers)[:,1]\n",
    "    wrong_ids = model.to_tokens(wrong_answers)[:,1]\n",
    "\n",
    "    return word_idxs, clean_prompts, corrupt_prompts, correct_ids, wrong_ids\n",
    "\n",
    "file_path = 'datasets/ioi/ioi_BABA_template_0_prompts.json'\n",
    "word_idxs, clean_prompts, corrupt_prompts, correct_ids, wrong_ids = load_batch(file_path, batch_size=10)\n",
    "\n",
    "print(\"Clean Prompts:\", clean_prompts)\n",
    "print(\"Corrupt Prompts:\", corrupt_prompts)\n",
    "print(\"Correct IDs:\", correct_ids)\n",
    "print(\"Wrong IDs:\", wrong_ids)\n",
    "\n",
    "\n",
    "S1 = word_idxs['S1']+1\n",
    "S2 = word_idxs['S2']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eindex import eindex\n",
    "def correct_logit(logits, correct_ids, wrong_ids):\n",
    "    return eindex(logits[:,-1], correct_ids, \"batch [batch]\").mean()\n",
    "\n",
    "def wrong_logit(logits, correct_ids, wrong_ids):\n",
    "    return eindex(logits[:,-1], wrong_ids, \"batch [batch]\").mean()\n",
    "\n",
    "def logit_diff(logits, correct_ids, wrong_ids):\n",
    "    return correct_logit(logits, correct_ids, wrong_ids) - wrong_logit(logits, correct_ids, wrong_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'IndirectLeap' from 'circuit_finder.patching.indirect_leap' (/root/circuit-finder/circuit_finder/patching/indirect_leap.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcircuit_finder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatching\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindirect_leap\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndirectLeap, LeapConfig\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mreset_hooks()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcircuit_finder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_html_graph\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'IndirectLeap' from 'circuit_finder.patching.indirect_leap' (/root/circuit-finder/circuit_finder/patching/indirect_leap.py)"
     ]
    }
   ],
   "source": [
    "from circuit_finder.patching.indirect_leap import IndirectLeap, LeapConfig\n",
    "model.reset_hooks()\n",
    "from circuit_finder.plotting import make_html_graph\n",
    "cfg = LEAPConfig(threshold=0.04,\n",
    "                 contrast_pairs=True, \n",
    "                 qk_enabled=True,\n",
    "                 chained_attribs=True,\n",
    "                 store_error_attribs=True,\n",
    "                 allow_neg_feature_acts=True)\n",
    "metric = partial(logit_diff, correct_ids=correct_ids, wrong_ids=wrong_ids)\n",
    "leap = IndirectLEAP(\n",
    "    cfg, clean_prompts, model, attn_saes, mlp_transcoders, metric, corrupt_tokens=corrupt_prompts\n",
    ")\n",
    "leap.run()\n",
    "print(\"num edges: \", len(leap.graph))\n",
    "graph = EAPGraph(leap.graph)\n",
    "types = graph.get_edge_types()\n",
    "print(\"num ov edges: \", len([t for t in types if t==\"ov\"]))\n",
    "print(\"num q edges: \", len([t for t in types if t==\"q\"]))\n",
    "print(\"num k edges: \", len([t for t in types if t==\"k\"]))\n",
    "print(\"num mlp edges: \", len([t for t in types if t==None]))\n",
    "make_html_graph(leap, attrib_type=\"em\", node_offset=8.0, show_error_nodes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we look at the graph for John and Mary, which is in BABA form. There are 3 feature-clusters:\n",
    "\n",
    "C1. Important attn features at the S2 position, around layer 5-6\n",
    "C2. An important attn feature at the final token position at layer 8.\n",
    "C3. A couple attn features at the final token position in layer 9-10, which strongly boost \"Mary\".\n",
    "\n",
    "We'd like to understand how these things compose. Namely:\n",
    "\n",
    "- How do the layer 9-10 attention heads \"know\" to attend more to Mary than John?\n",
    "    - They have high key-attribution to C2 features. But how exactly do these C2 features tell the model where to attend to? Do they mainly boost attention to tokens with the IO property? Or do they suppress attention to tokens with the S property? (Hunch: IO and S isn't really the right way to think here - it's really about \"name that was repeated\" and \"name that wasn't repeated\".)\n",
    "- How are the C2 features computed?\n",
    "    - Hunch: L8 attention uses the \"proper noun should come next\" feature as a query, the C1 features act as a key, and the OV circuit applied to C1 gives C2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at the 3 main C1 features:\n",
    "\n",
    "- https://www.neuronpedia.org/gpt2-small/5-att-kk/7515\n",
    "- https://www.neuronpedia.org/gpt2-small/6-att-kk/13836\n",
    "- https://www.neuronpedia.org/gpt2-small/6-att-kk/17410\n",
    "\n",
    "All three features are basically \"token that was previously followed by 'and'\" features.\n",
    "\n",
    "e.g. \"The shirt is blue and white. Her eyes are |blue|\"\n",
    "\n",
    "Let's ablate all three of them and see what happens to the logit diff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, clean_cache = run_with_ablations(\n",
    "    model,\n",
    "    clean_prompts,\n",
    "    attn_saes,\n",
    "    mlp_transcoders,\n",
    "    ablation_list = [],\n",
    "    patch_list = [],\n",
    "    cache_names_filter = [f'blocks.{layer}.attn.hook_pattern' for layer in range(12)]\n",
    ")\n",
    "clean_logit_diff = logit_diff(logits, correct_ids, wrong_ids).item()\n",
    "logits, ablated_cache = run_with_ablations(\n",
    "    model,\n",
    "    clean_prompts,\n",
    "    attn_saes,\n",
    "    mlp_transcoders,\n",
    "    ablation_list = [(\"attn\", 5, 7515, S2),\n",
    "                     (\"attn\", 6, 17410, S2),\n",
    "                     (\"attn\", 6, 13836, S2)],\n",
    "    patch_list = [],\n",
    "    cache_names_filter = [f'blocks.{layer}.attn.hook_pattern' for layer in range(12)]\n",
    ")\n",
    "ablated_logit_diff = logit_diff(logits, correct_ids, wrong_ids).item()\n",
    "print()\n",
    "print(f\"clean logit diff = {clean_logit_diff:.2f}\")\n",
    "print(f\"ablated logit diff = {ablated_logit_diff:.2f}\")\n",
    "print(f\"percent logit diff reduction = {100*(clean_logit_diff-ablated_logit_diff)/clean_logit_diff :.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ablating these three nodes reduces the metric by more than a third! Now, let's work out *why*. First, we can look at how these ablations affect the attention pattern from the last token back to the previous positions, in the layers that matter (8, 9 and 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "for layer in [7,8,9,10,11]:\n",
    "    A = clean_cache[f'blocks.{layer}.attn.hook_pattern'].mean([0,1])[-1].cpu()\n",
    "    B = ablated_cache[f'blocks.{layer}.attn.hook_pattern'].mean([0,1])[-1].cpu()\n",
    "    C = model.to_str_tokens(clean_prompts[0])\n",
    "\n",
    "    # Create unique labels\n",
    "    counter = Counter(C)\n",
    "    unique_labels = []\n",
    "    for label in C:\n",
    "        count = counter[label]\n",
    "        if count > 1:\n",
    "            unique_labels.append(f\"{label} \")\n",
    "            counter[label] -= 1\n",
    "        else:\n",
    "            unique_labels.append(label)\n",
    "\n",
    "    # Convert tensors to pandas DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'tokens': unique_labels,\n",
    "        'clean': A.numpy(),\n",
    "        'ablated': B.numpy()\n",
    "    })\n",
    "\n",
    "    # Melt the DataFrame to long format\n",
    "    df_melted = df.melt(id_vars='tokens', value_vars=['clean', 'ablated'], var_name='Tensor', value_name='attention prob')\n",
    "\n",
    "    # Create the bar chart with barmode set to 'group'\n",
    "    fig = px.bar(df_melted, x='tokens', y='attention prob', color='Tensor', barmode='group', title=f'Layer {layer} Attention')\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The C1 ablations change the layer 9 and 10 patterns by a bit, bit not much: they only change the attention probs on the name tokens by ~10%.\n",
    "\n",
    "We can also subtract off the C1 feature components at various points in the residual stream, and check the effect on the logit diff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_logit_diff_reductions = []\n",
    "layers = [7,8,9,10,11]\n",
    "for patch_layer in layers:\n",
    "    model.reset_hooks()\n",
    "    logits, ablated_cache = run_with_ablations(\n",
    "        model,\n",
    "        clean_prompts,\n",
    "        attn_saes,\n",
    "        mlp_transcoders,\n",
    "        grab_and_delete_list = [\n",
    "            (\"attn\", 5, 7515, S2, f'blocks.{patch_layer}.hook_resid_pre'),\n",
    "            (\"attn\", 6, 17410, S2, f'blocks.{patch_layer}.hook_resid_pre'),\n",
    "            (\"attn\", 6, 13836, S2, f'blocks.{patch_layer}.hook_resid_pre')\n",
    "        ],\n",
    "        cache_names_filter = [f'blocks.{layer}.attn.hook_pattern' for layer in range(12)]\n",
    "    )\n",
    "    reduction = (clean_logit_diff - logit_diff(logits, correct_ids, wrong_ids).item()) / clean_logit_diff\n",
    "    percent_logit_diff_reductions += [100*reduction]\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'layer': layers,\n",
    "    'logit diff percent reduction': percent_logit_diff_reductions\n",
    "})\n",
    "px.line(df, x='layer', y='logit diff percent reduction', \n",
    "        title=\"Effect of deleting C1 features at resid_pre\",\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BAAB S2 features:\n",
    "\n",
    "https://www.neuronpedia.org/gpt2-small/8-att-kk/5580\n",
    "\n",
    "\n",
    "BAAB final pos key features:\n",
    "\n",
    "https://www.neuronpedia.org/gpt2-small/8-att-kk/13676 works with both orderings\n",
    "\n",
    "https://www.neuronpedia.org/gpt2-small/7-att-kk/3607 another CS feature\n",
    "\n",
    "https://www.neuronpedia.org/gpt2-small/8-att-kk/5580  e.g. A said \"..... |,\"|. Model predicts A.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
