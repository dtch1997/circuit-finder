{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8: ignore\n",
    "\"\"\"Script to run an experiment with LEAP\n",
    "\n",
    "Usage:\n",
    "pdm run python -m circuit_finder.experiments.run_leap_experiment [ARGS]\n",
    "\n",
    "Run with flag '-h', '--help' to see the arguments.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/workspace/circuit-finder\")\n",
    "\n",
    "import pathlib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import transformer_lens as tl\n",
    "\n",
    "from simple_parsing import ArgumentParser\n",
    "from dataclasses import dataclass\n",
    "from circuit_finder.patching.eap_graph import EAPGraph\n",
    "from circuit_finder.utils import clear_memory\n",
    "from circuit_finder.patching.ablate import get_metric_with_ablation\n",
    "from circuit_finder.data_loader import load_datasets_from_json, PromptPairBatch\n",
    "from circuit_finder.constants import device\n",
    "from tqdm import tqdm\n",
    "from circuit_finder.patching.ablate import get_metric_with_ablation\n",
    "\n",
    "from typing import Literal\n",
    "from eindex import eindex\n",
    "from pathlib import Path\n",
    "from circuit_finder.pretrained import (\n",
    "    load_model,\n",
    "    load_attn_saes,\n",
    "    load_hooked_mlp_transcoders,\n",
    ")\n",
    "from circuit_finder.patching.indirect_leap import (\n",
    "    preprocess_attn_saes,\n",
    "    IndirectLEAP,\n",
    "    LEAPConfig,\n",
    ")\n",
    "from circuit_finder.core.types import Model\n",
    "from circuit_finder.metrics import batch_avg_answer_diff\n",
    "from circuit_finder.constants import ProjectDir\n",
    "from circuit_finder.patching.ablate import (\n",
    "    splice_model_with_saes_and_transcoders,\n",
    "    get_metric_with_ablation,\n",
    "    AblateType,\n",
    ")\n",
    "from circuit_finder.plotting import make_html_graph\n",
    "\n",
    "\n",
    "def batch_to_str_dict(batch: PromptPairBatch, model):\n",
    "    return {\n",
    "        \"clean\": model.to_string(batch.clean),\n",
    "        \"answer\": model.to_string(batch.answers),\n",
    "        \"wrong_answer\": model.to_string(batch.wrong_answers),\n",
    "        \"corrupt\": model.to_string(batch.corrupt),\n",
    "    }\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CleanCorruptExample:\n",
    "    clean_prompt: str\n",
    "    answer: str\n",
    "    wrong_answer: str\n",
    "    corrupt_prompt: str\n",
    "\n",
    "    def clean_tokens(self, model: Model):\n",
    "        return model.to_tokens(self.clean_prompt)\n",
    "\n",
    "    def answer_tokens(self, model: Model):\n",
    "        return model.to_tokens(self.answer, prepend_bos=False).squeeze(0)\n",
    "\n",
    "    def wrong_answer_tokens(self, model: Model):\n",
    "        return model.to_tokens(self.wrong_answer, prepend_bos=False).squeeze(0)\n",
    "\n",
    "    def corrupt_tokens(self, model: Model):\n",
    "        return model.to_tokens(self.corrupt_prompt)\n",
    "\n",
    "\n",
    "datasets: dict[str, CleanCorruptExample] = {\n",
    "    # Datasets reported in \"Have Faith in Faithfulness\" (Hanna et al, 2023)\n",
    "    \"gender-bias\": CleanCorruptExample(\n",
    "        clean_prompt=\"The doctor is ready, you can go see\",\n",
    "        answer=\" him\",\n",
    "        wrong_answer=\" her\",\n",
    "        corrupt_prompt=\"The nurse is ready, you can go see\",\n",
    "    ),\n",
    "    # TODO: Subject-verb Agreement\n",
    "    \"ioi\": CleanCorruptExample(\n",
    "        clean_prompt=\"When John and Mary went to the shop, John gave a bottle to\",\n",
    "        answer=\" Mary\",\n",
    "        wrong_answer=\" John\",\n",
    "        corrupt_prompt=\"When Alice and Bob went to the shop, Charlie gave a bottle to\",\n",
    "    ),\n",
    "    # TODO: Hypernymy\n",
    "    # TODO: Greater-than\n",
    "    # Datasets Jacob came up with himself\n",
    "    \"for-loop\": CleanCorruptExample(\n",
    "        clean_prompt=\"for y in x: print(\",\n",
    "        answer=\"y\",\n",
    "        wrong_answer=\"x\",\n",
    "        corrupt_prompt=\"for x in y: print(\",\n",
    "    ),\n",
    "    # \"if-else\": CleanCorruptExample(\n",
    "    #     clean_prompt=\"if x>5: print(x)\",\n",
    "    #     answer=\"else\",\n",
    "    #     wrong_answer=\"if\",\n",
    "    #     corrupt_prompt=\"while x>5: print(x)\",\n",
    "    # ),\n",
    "}\n",
    "\n",
    "\n",
    "def load_models():\n",
    "    model = load_model()\n",
    "    attn_sae_dict = load_attn_saes()\n",
    "    # TODO: get rid of need to preprocess attn saes\n",
    "    attn_sae_dict = preprocess_attn_saes(attn_sae_dict, model)\n",
    "    hooked_mlp_transcoder_dict = load_hooked_mlp_transcoders()\n",
    "    attn_saes = list(attn_sae_dict.values())\n",
    "    transcoders = list(hooked_mlp_transcoder_dict.values())\n",
    "\n",
    "    return model, attn_saes, transcoders\n",
    "\n",
    "\n",
    "def compute_logit_diff(model, clean_tokens, answer_tokens, wrong_answer_tokens):\n",
    "    clean_logits = model(clean_tokens)\n",
    "    last_logits = clean_logits[:, -1, :]\n",
    "    correct_logits = eindex(last_logits, answer_tokens, \"batch [batch]\")\n",
    "    wrong_logits = eindex(last_logits, wrong_answer_tokens, \"batch [batch]\")\n",
    "    return correct_logits - wrong_logits\n",
    "\n",
    "\n",
    "def get_clean_and_corrupt_metric(\n",
    "    model,\n",
    "    metric_fn,\n",
    "    clean_tokens,\n",
    "    corrupt_tokens,\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        clean_metric = metric_fn(model, clean_tokens).item()\n",
    "        corrupt_metric = metric_fn(model, corrupt_tokens).item()\n",
    "\n",
    "    return clean_metric, corrupt_metric\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LeapExperimentConfig:\n",
    "    leap_config: LEAPConfig\n",
    "    feature_ablate_type: AblateType = \"value\"\n",
    "    error_ablate_type: AblateType = None\n",
    "    first_ablated_layer: int = 2\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class LeapExperimentResult:\n",
    "    config: LEAPConfig\n",
    "    clean_metric: float\n",
    "    ablate_metric: float\n",
    "    corrupt_metric: float\n",
    "    graph: EAPGraph\n",
    "    error_graph: EAPGraph\n",
    "\n",
    "\n",
    "def run_leap(\n",
    "    leap_config: LEAPConfig,\n",
    "    batch: PromptPairBatch,\n",
    "    model: Model,\n",
    "    attn_saes,\n",
    "    hooked_mlp_transcoders,\n",
    "    ablate_cache: tl.ActivationCache,\n",
    "    *,\n",
    "    save_dir: pathlib.Path,\n",
    "    metric_fn_name: str = \"logit_diff\",\n",
    "    feature_ablate_type: AblateType = \"value\",\n",
    "    error_ablate_type: AblateType = None,\n",
    "    first_ablated_layer: int = 2,\n",
    "):\n",
    "    # Setup the tokens\n",
    "    clean_tokens = batch.clean\n",
    "    answer_tokens = batch.answers\n",
    "    wrong_answer_tokens = batch.wrong_answers\n",
    "    corrupt_tokens = batch.corrupt\n",
    "\n",
    "    # Save the batch\n",
    "    batch_dict = batch_to_str_dict(batch, model)\n",
    "    with open(save_dir / \"batch.json\", \"w\") as f:\n",
    "        json.dump(batch_dict, f)\n",
    "\n",
    "    # Setup the metric function\n",
    "    if metric_fn_name == \"logit_diff\":\n",
    "\n",
    "        def metric_fn(model, tokens):\n",
    "            # Get the last-token logits\n",
    "            logits = model(tokens)[:, -1, :]\n",
    "            logit_diff = batch_avg_answer_diff(logits, batch)\n",
    "            return logit_diff.mean()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown metric_fn_name: {metric_fn_name}\")\n",
    "\n",
    "    clean_metric, corrupt_metric = get_clean_and_corrupt_metric(\n",
    "        model, metric_fn, clean_tokens, corrupt_tokens\n",
    "    )\n",
    "\n",
    "    # Run LEAP\n",
    "    leap = IndirectLEAP(\n",
    "        cfg=leap_config,\n",
    "        tokens=clean_tokens,\n",
    "        model=model,\n",
    "        metric=metric_fn,\n",
    "        attn_saes=attn_saes,  # type: ignore\n",
    "        transcoders=hooked_mlp_transcoders,\n",
    "        corrupt_tokens=corrupt_tokens,\n",
    "    )\n",
    "    leap.run()\n",
    "    graph = EAPGraph(leap.graph)\n",
    "    error_graph = EAPGraph(leap.error_graph)\n",
    "\n",
    "    # Save the graph\n",
    "    make_html_graph(leap, html_file=str(save_dir.absolute() / \"graph.html\"))\n",
    "\n",
    "    # Run ablation experiment\n",
    "    ablate_metric = get_metric_with_ablation(\n",
    "        model,  # type: ignore\n",
    "        graph,\n",
    "        clean_tokens,\n",
    "        metric_fn,\n",
    "        hooked_mlp_transcoders,\n",
    "        attn_saes,\n",
    "        ablate_cache,\n",
    "        feature_ablate_type=feature_ablate_type,\n",
    "        error_ablate_type=error_ablate_type,\n",
    "        first_ablated_layer=first_ablated_layer,\n",
    "    ).item()\n",
    "\n",
    "    # TODO: noising, denoising plots.\n",
    "\n",
    "    return LeapExperimentResult(\n",
    "        config=leap_config,\n",
    "        clean_metric=clean_metric,\n",
    "        ablate_metric=ablate_metric,\n",
    "        corrupt_metric=corrupt_metric,\n",
    "        graph=graph,\n",
    "        error_graph=error_graph,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/circuit-finder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b893b31f958e46a2a93e5f8845fee628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "from circuit_finder.data.ioi import ABBA_DATASETS, BABA_DATASETS\n",
    "\n",
    "IOI_DATASETS = ABBA_DATASETS[:2] + BABA_DATASETS[:2]\n",
    "THRESHOLDS = [0.001, 0.003, 0.006, 0.01, 0.03, 0.06]\n",
    "# THRESHOLDS = [0.03]\n",
    "\n",
    "model = load_model()\n",
    "attn_saes = load_attn_saes()\n",
    "attn_saes = preprocess_attn_saes(attn_saes, model)\n",
    "hooked_mlp_transcoders = load_hooked_mlp_transcoders()\n",
    "metric_fn_name = \"logit_diff\"\n",
    "batch_size = 1\n",
    "\n",
    "with open(ProjectDir / \"data\" / \"c4_mean_acts.pkl\", \"rb\") as file:\n",
    "    ablate_cache = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = LEAPConfig(\n",
    "    threshold=0.001,\n",
    "    contrast_pairs=True,\n",
    "    qk_enabled=True,\n",
    "    chained_attribs=True,\n",
    "    allow_neg_feature_acts=True,\n",
    "    store_error_attribs=True,\n",
    ")\n",
    "experiment_cfg = LeapExperimentConfig(\n",
    "    leap_config=cfg,\n",
    "    feature_ablate_type=\"value\",\n",
    "    error_ablate_type=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment on dataset: ioi_ABBA_template_0_prompts\n",
      "/workspace/circuit-finder/leap_experiment_results/dataset=ioi_ABBA_template_0_prompts_threshold=0.001_batch-size=1/graph.html\n",
      "Generated /workspace/circuit-finder/leap_experiment_results/dataset=ioi_ABBA_template_0_prompts_threshold=0.001_batch-size=1/graph.html. Open this file in Live Server to view the graph.\n",
      "Clean Metric: 4.267217636108398\n",
      "Ablate Metric: -0.04824018478393555\n",
      "Corrupt Metric: 1.609701156616211\n",
      "Running experiment on dataset: ioi_ABBA_template_0_prompts\n",
      "/workspace/circuit-finder/leap_experiment_results/dataset=ioi_ABBA_template_0_prompts_threshold=0.003_batch-size=1/graph.html\n",
      "Generated /workspace/circuit-finder/leap_experiment_results/dataset=ioi_ABBA_template_0_prompts_threshold=0.003_batch-size=1/graph.html. Open this file in Live Server to view the graph.\n",
      "Clean Metric: 4.267217636108398\n",
      "Ablate Metric: -0.16108942031860352\n",
      "Corrupt Metric: 1.609701156616211\n",
      "Running experiment on dataset: ioi_ABBA_template_0_prompts\n",
      "/workspace/circuit-finder/leap_experiment_results/dataset=ioi_ABBA_template_0_prompts_threshold=0.006_batch-size=1/graph.html\n",
      "Generated /workspace/circuit-finder/leap_experiment_results/dataset=ioi_ABBA_template_0_prompts_threshold=0.006_batch-size=1/graph.html. Open this file in Live Server to view the graph.\n",
      "Clean Metric: 4.267217636108398\n",
      "Ablate Metric: -0.14775466918945312\n",
      "Corrupt Metric: 1.609701156616211\n",
      "Running experiment on dataset: ioi_ABBA_template_0_prompts\n",
      "/workspace/circuit-finder/leap_experiment_results/dataset=ioi_ABBA_template_0_prompts_threshold=0.01_batch-size=1/graph.html\n",
      "Generated /workspace/circuit-finder/leap_experiment_results/dataset=ioi_ABBA_template_0_prompts_threshold=0.01_batch-size=1/graph.html. Open this file in Live Server to view the graph.\n",
      "Clean Metric: 4.267217636108398\n",
      "Ablate Metric: -0.006203651428222656\n",
      "Corrupt Metric: 1.609701156616211\n",
      "Running experiment on dataset: ioi_ABBA_template_0_prompts\n",
      "/workspace/circuit-finder/leap_experiment_results/dataset=ioi_ABBA_template_0_prompts_threshold=0.03_batch-size=1/graph.html\n",
      "Generated /workspace/circuit-finder/leap_experiment_results/dataset=ioi_ABBA_template_0_prompts_threshold=0.03_batch-size=1/graph.html. Open this file in Live Server to view the graph.\n",
      "Clean Metric: 4.267217636108398\n",
      "Ablate Metric: 0.1328887939453125\n",
      "Corrupt Metric: 1.609701156616211\n",
      "Running experiment on dataset: ioi_ABBA_template_0_prompts\n",
      "/workspace/circuit-finder/leap_experiment_results/dataset=ioi_ABBA_template_0_prompts_threshold=0.06_batch-size=1/graph.html\n",
      "Generated /workspace/circuit-finder/leap_experiment_results/dataset=ioi_ABBA_template_0_prompts_threshold=0.06_batch-size=1/graph.html. Open this file in Live Server to view the graph.\n",
      "Clean Metric: 4.267217636108398\n",
      "Ablate Metric: 0.14666461944580078\n",
      "Corrupt Metric: 1.609701156616211\n"
     ]
    }
   ],
   "source": [
    "# Sweep over datasets\n",
    "for dataset_path in IOI_DATASETS:\n",
    "    dataset_name = pathlib.Path(dataset_path).stem\n",
    "    train_loader, test_loader = load_datasets_from_json(\n",
    "        model,\n",
    "        ProjectDir / dataset_path,\n",
    "        device=torch.device(\"cuda\"),\n",
    "        batch_size=batch_size,\n",
    "        train_test_size=(batch_size, batch_size),\n",
    "    )\n",
    "    batch = next(iter(train_loader))\n",
    "\n",
    "    # Sweep over thresholds\n",
    "    for threshold in THRESHOLDS:\n",
    "        # Main script\n",
    "        cfg.threshold = threshold\n",
    "        save_dir = (\n",
    "            ProjectDir\n",
    "            / \"leap_experiment_results\"\n",
    "            / f\"dataset={dataset_name}_threshold={threshold}_batch-size={batch_size}\"\n",
    "        )\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(f\"Running experiment on dataset: {dataset_name}\")\n",
    "        leap_experiment_result = run_leap(\n",
    "            cfg,\n",
    "            batch,\n",
    "            model,\n",
    "            attn_saes,\n",
    "            hooked_mlp_transcoders,\n",
    "            ablate_cache,\n",
    "            save_dir=save_dir,\n",
    "            metric_fn_name=metric_fn_name,\n",
    "            feature_ablate_type=experiment_cfg.feature_ablate_type,\n",
    "            error_ablate_type=experiment_cfg.error_ablate_type,\n",
    "            first_ablated_layer=experiment_cfg.first_ablated_layer,\n",
    "        )\n",
    "\n",
    "        if leap_experiment_result is None:\n",
    "            continue\n",
    "\n",
    "        print(f\"Clean Metric: {leap_experiment_result.clean_metric}\")\n",
    "        print(f\"Ablate Metric: {leap_experiment_result.ablate_metric}\")\n",
    "        print(f\"Corrupt Metric: {leap_experiment_result.corrupt_metric}\")\n",
    "\n",
    "        with open(save_dir / \"result.pkl\", \"wb\") as f:\n",
    "            pickle.dump(leap_experiment_result, f)\n",
    "\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
