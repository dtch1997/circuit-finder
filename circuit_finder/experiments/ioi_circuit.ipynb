{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"/home/daniel/ml_workspace/circuit-finder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/ml_workspace/circuit-finder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from circuit_finder.pretrained import load_model\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77430c3ce7074224aaf9693826f5092c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from circuit_finder.pretrained import load_attn_saes, load_hooked_mlp_transcoders\n",
    "from circuit_finder.patching.indirect_leap import preprocess_attn_saes\n",
    "\n",
    "attn_sae_dict = load_attn_saes()\n",
    "attn_sae_dict = preprocess_attn_saes(attn_sae_dict, model)\n",
    "hooked_mlp_transcoder_dict = load_hooked_mlp_transcoders()\n",
    "\n",
    "attn_saes = list(attn_sae_dict.values())\n",
    "transcoders = list(hooked_mlp_transcoder_dict.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = \"When John and Mary went to the shop, John gave a bottle to\"\n",
    "answer = \" Mary\"\n",
    "wrong_answer = \" John\"\n",
    "corrupt_text = \"When Alice and Bob went to the shop, Charlie gave a bottle to\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "clean_tokens = model.to_tokens(clean_text)\n",
    "answer_tokens = model.to_tokens(answer, prepend_bos=False).squeeze(-1)\n",
    "wrong_answer_tokens = model.to_tokens(wrong_answer, prepend_bos=False).squeeze(-1)\n",
    "corrupt_tokens = model.to_tokens(corrupt_text)\n",
    "\n",
    "print(clean_tokens.shape)\n",
    "print(answer_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.373994827270508\n",
      "1.0941038131713867\n",
      "2.8041763305664062\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from eindex import eindex\n",
    "from circuit_finder.patching.eap_graph import EAPGraph\n",
    "from circuit_finder.patching.ablate import get_metric_with_ablation\n",
    "from circuit_finder.patching.indirect_leap import IndirectLEAP, LEAPConfig\n",
    "from circuit_finder.utils import clear_memory\n",
    "\n",
    "ablate_tokens = corrupt_tokens\n",
    "\n",
    "def compute_logit_diff(model, clean_tokens, answer_tokens, wrong_answer_tokens):\n",
    "    clean_logits = model(clean_tokens)\n",
    "    last_logits = clean_logits[:, -1, :]\n",
    "    correct_logits = eindex(last_logits, answer_tokens, \"batch [batch]\")\n",
    "    wrong_logits = eindex(last_logits, wrong_answer_tokens, \"batch [batch]\")\n",
    "    return correct_logits - wrong_logits\n",
    "\n",
    "def metric_fn(model, tokens):\n",
    "    logit_diff = compute_logit_diff(model, tokens, answer_tokens, wrong_answer_tokens)\n",
    "    return logit_diff.mean()\n",
    "\n",
    "# NOTE: First, get the ceiling of the patching metric.\n",
    "# TODO: Replace 'last_token_logit' with logit difference\n",
    "with torch.no_grad():\n",
    "    ceiling = metric_fn(model, clean_tokens).item()\n",
    "print(ceiling)\n",
    "\n",
    "# NOTE: Second, get floor of patching metric using empty graph, i.e. ablate everything\n",
    "with torch.no_grad():\n",
    "    empty_graph = EAPGraph([])\n",
    "    floor = get_metric_with_ablation(\n",
    "        model,\n",
    "        empty_graph,\n",
    "        clean_tokens,\n",
    "        metric_fn,\n",
    "        hooked_mlp_transcoder_dict,\n",
    "        attn_sae_dict,\n",
    "        ablate_nodes=\"bm\",\n",
    "        ablate_errors=False,  # Do not ablate errors when running forward pass\n",
    "        first_ablated_layer=2,\n",
    "        corrupt_tokens = ablate_tokens,\n",
    "    ).item()\n",
    "clear_memory()\n",
    "print(floor)\n",
    "\n",
    "\n",
    "# now sweep over thresholds to get graphs with variety of numbers of nodes\n",
    "# for each graph we calculate faithfulness\n",
    "num_nodes_list = []\n",
    "metrics_list = []\n",
    "\n",
    "# Sweep over thresholds\n",
    "# TODO: make configurable\n",
    "# thresholds = [0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1.0]\n",
    "thresholds = [0.01]\n",
    "for threshold in thresholds:\n",
    "    # Setup LEAP algorithm\n",
    "    model.reset_hooks()\n",
    "    cfg = LEAPConfig(\n",
    "        threshold=threshold, contrast_pairs=True, chained_attribs=True\n",
    "    )\n",
    "    leap = IndirectLEAP(\n",
    "        cfg=cfg,\n",
    "        tokens=clean_tokens,\n",
    "        model=model,\n",
    "        metric=metric_fn,\n",
    "        attn_saes=attn_saes,  # type: ignore\n",
    "        transcoders=transcoders,\n",
    "        corrupt_tokens=ablate_tokens,\n",
    "    )\n",
    "\n",
    "    # Populate the graph\n",
    "    leap.metric_step()\n",
    "    for layer in reversed(range(1, leap.n_layers)):\n",
    "        leap.mlp_step(layer)\n",
    "        leap.ov_step(layer)\n",
    "\n",
    "    # Save the graph\n",
    "    graph = EAPGraph(leap.graph)\n",
    "    num_nodes = len(graph.get_src_nodes())\n",
    "\n",
    "    # Delete tensors to save memory\n",
    "    del leap\n",
    "    clear_memory()\n",
    "\n",
    "    # # Calculate the metric under ablation\n",
    "    with torch.no_grad():\n",
    "        metric = get_metric_with_ablation(\n",
    "            model,\n",
    "            graph,\n",
    "            clean_tokens,\n",
    "            metric_fn,\n",
    "            hooked_mlp_transcoder_dict,\n",
    "            attn_sae_dict,\n",
    "            ablate_nodes=\"bm\",\n",
    "            ablate_errors=False,\n",
    "            first_ablated_layer=2,\n",
    "            corrupt_tokens = ablate_tokens,\n",
    "        ).item()\n",
    "    clear_memory()\n",
    "    print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267\n",
      "graph.html\n",
      "Generated graph.html. Open this file in Live Server to view the graph.\n"
     ]
    }
   ],
   "source": [
    "from circuit_finder.plotting import make_html_graph\n",
    "\n",
    "print(len(graph.get_edges()))\n",
    "make_html_graph(graph, tokens = model.to_str_tokens(clean_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
