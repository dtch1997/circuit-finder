{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/workspace/circuit-finder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/circuit-finder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59ac95c3d774554813360682106f07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from circuit_finder.pretrained import (\n",
    "    load_model,\n",
    "    load_attn_saes,\n",
    "    load_hooked_mlp_transcoders,\n",
    ")\n",
    "from circuit_finder.patching.indirect_leap import preprocess_attn_saes\n",
    "\n",
    "model = load_model()\n",
    "attn_saes = load_attn_saes()\n",
    "attn_saes = preprocess_attn_saes(attn_saes, model)\n",
    "hooked_mlp_transcoders = load_hooked_mlp_transcoders()\n",
    "\n",
    "transcoders = list(hooked_mlp_transcoders.values())\n",
    "saes = list(attn_saes.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/circuit-finder/.venv/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for c4 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/c4\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/root/.cache/huggingface/modules/datasets_modules/datasets/c4/584d57ebe81c209b6c7f31727066d2c4b4bba37cb7092cdd83083d5ec11207db/c4.py:53: FutureWarning: Dataset 'c4' is deprecated and will be deleted. Use 'allenai/c4' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"c4\", \"en\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import ActivationCache\n",
    "from circuit_finder.patching.ablate import (\n",
    "    splice_model_with_saes_and_transcoders,\n",
    "    filter_sae_acts_and_errors,\n",
    ")\n",
    "\n",
    "n_tokens = 0\n",
    "# total_tokens = 100_000 # 100k tokens\n",
    "total_tokens = 100\n",
    "print(f\"Total tokens: {total_tokens}\")\n",
    "\n",
    "# A bit of a hack, run once to get cache shapes\n",
    "with splice_model_with_saes_and_transcoders(model, transcoders, saes):\n",
    "    _, dummy_cache = model.run_with_cache(\n",
    "        \"Hello World\", names_filter=filter_sae_acts_and_errors\n",
    "    )\n",
    "\n",
    "\n",
    "zero_cache_dict = {\n",
    "    hook_name: torch.zeros_like(act.sum(1).squeeze(0))\n",
    "    for hook_name, act in dummy_cache.items()\n",
    "}\n",
    "# zero_cache = ActivationCache(zero_cache_dict, model)\n",
    "\n",
    "# Run the model\n",
    "with splice_model_with_saes_and_transcoders(model, transcoders, saes):\n",
    "    for element in dataset[\"train\"]:\n",
    "        text = element[\"text\"]\n",
    "        tokens = model.to_tokens(text)\n",
    "        _, cache = model.run_with_cache(text, names_filter=filter_sae_acts_and_errors)\n",
    "\n",
    "        n_tokens += tokens.shape[1]\n",
    "        for hook_name, act in cache.items():\n",
    "            zero_cache_dict[hook_name] += act.sum(1).squeeze(0)\n",
    "\n",
    "        if n_tokens >= total_tokens:\n",
    "            break\n",
    "\n",
    "# Average the cache\n",
    "for hook_name, act in zero_cache_dict.items():\n",
    "    zero_cache_dict[hook_name] /= n_tokens\n",
    "\n",
    "zero_cache = ActivationCache(zero_cache_dict, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(zero_cache[\"blocks.0.attn.hook_z.hook_sae_acts_post\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.attn.hook_z.hook_sae_acts_post torch.Size([49152])\n",
      "blocks.0.attn.hook_z.hook_sae_error torch.Size([768])\n",
      "blocks.0.mlp.transcoder.hook_sae_acts_post torch.Size([24576])\n",
      "blocks.0.mlp.hook_sae_error torch.Size([768])\n",
      "blocks.1.attn.hook_z.hook_sae_acts_post torch.Size([49152])\n",
      "blocks.1.attn.hook_z.hook_sae_error torch.Size([768])\n",
      "blocks.1.mlp.transcoder.hook_sae_acts_post torch.Size([24576])\n",
      "blocks.1.mlp.hook_sae_error torch.Size([768])\n",
      "blocks.2.attn.hook_z.hook_sae_acts_post torch.Size([49152])\n",
      "blocks.2.attn.hook_z.hook_sae_error torch.Size([768])\n",
      "blocks.2.mlp.transcoder.hook_sae_acts_post torch.Size([24576])\n",
      "blocks.2.mlp.hook_sae_error torch.Size([768])\n",
      "blocks.3.attn.hook_z.hook_sae_acts_post torch.Size([49152])\n",
      "blocks.3.attn.hook_z.hook_sae_error torch.Size([768])\n",
      "blocks.3.mlp.transcoder.hook_sae_acts_post torch.Size([24576])\n",
      "blocks.3.mlp.hook_sae_error torch.Size([768])\n",
      "blocks.4.attn.hook_z.hook_sae_acts_post torch.Size([49152])\n",
      "blocks.4.attn.hook_z.hook_sae_error torch.Size([768])\n",
      "blocks.4.mlp.transcoder.hook_sae_acts_post torch.Size([24576])\n",
      "blocks.4.mlp.hook_sae_error torch.Size([768])\n",
      "blocks.5.attn.hook_z.hook_sae_acts_post torch.Size([49152])\n",
      "blocks.5.attn.hook_z.hook_sae_error torch.Size([768])\n",
      "blocks.5.mlp.transcoder.hook_sae_acts_post torch.Size([24576])\n",
      "blocks.5.mlp.hook_sae_error torch.Size([768])\n",
      "blocks.6.attn.hook_z.hook_sae_acts_post torch.Size([49152])\n",
      "blocks.6.attn.hook_z.hook_sae_error torch.Size([768])\n",
      "blocks.6.mlp.transcoder.hook_sae_acts_post torch.Size([24576])\n",
      "blocks.6.mlp.hook_sae_error torch.Size([768])\n",
      "blocks.7.attn.hook_z.hook_sae_acts_post torch.Size([49152])\n",
      "blocks.7.attn.hook_z.hook_sae_error torch.Size([768])\n",
      "blocks.7.mlp.transcoder.hook_sae_acts_post torch.Size([24576])\n",
      "blocks.7.mlp.hook_sae_error torch.Size([768])\n",
      "blocks.8.attn.hook_z.hook_sae_acts_post torch.Size([49152])\n",
      "blocks.8.attn.hook_z.hook_sae_error torch.Size([768])\n",
      "blocks.8.mlp.transcoder.hook_sae_acts_post torch.Size([24576])\n",
      "blocks.8.mlp.hook_sae_error torch.Size([768])\n",
      "blocks.9.attn.hook_z.hook_sae_acts_post torch.Size([49152])\n",
      "blocks.9.attn.hook_z.hook_sae_error torch.Size([768])\n",
      "blocks.9.mlp.transcoder.hook_sae_acts_post torch.Size([24576])\n",
      "blocks.9.mlp.hook_sae_error torch.Size([768])\n",
      "blocks.10.attn.hook_z.hook_sae_acts_post torch.Size([49152])\n",
      "blocks.10.attn.hook_z.hook_sae_error torch.Size([768])\n",
      "blocks.10.mlp.transcoder.hook_sae_acts_post torch.Size([24576])\n",
      "blocks.10.mlp.hook_sae_error torch.Size([768])\n",
      "blocks.11.attn.hook_z.hook_sae_acts_post torch.Size([49152])\n",
      "blocks.11.attn.hook_z.hook_sae_error torch.Size([768])\n",
      "blocks.11.mlp.transcoder.hook_sae_acts_post torch.Size([24576])\n",
      "blocks.11.mlp.hook_sae_error torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for hook_name, act in zero_cache.items():\n",
    "    print(hook_name, act.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cache  \n",
    "\n",
    "import pickle\n",
    "with open(\"c4_mean_acts.pkl\", 'wb') as file:\n",
    "    pickle.dump(zero_cache, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Circuit Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/greaterthan_gpt2-small_prompts.json', 'datasets/ioi/ioi_ABBA_template_0_prompts.json', 'datasets/ioi/ioi_ABBA_template_1_prompts.json', 'datasets/ioi/ioi_BABA_template_0_prompts.json', 'datasets/ioi/ioi_BABA_template_1_prompts.json']\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import transformer_lens as tl\n",
    "\n",
    "from simple_parsing import ArgumentParser\n",
    "from dataclasses import dataclass\n",
    "from circuit_finder.patching.eap_graph import EAPGraph\n",
    "from circuit_finder.utils import clear_memory\n",
    "from circuit_finder.patching.ablate import get_metric_with_ablation\n",
    "from circuit_finder.data_loader import load_datasets_from_json, PromptPairBatch\n",
    "from circuit_finder.constants import device\n",
    "from tqdm import tqdm\n",
    "from circuit_finder.patching.ablate import get_metric_with_ablation\n",
    "\n",
    "from typing import Literal\n",
    "from eindex import eindex\n",
    "from pathlib import Path\n",
    "from circuit_finder.pretrained import (\n",
    "    load_model,\n",
    "    load_attn_saes,\n",
    "    load_hooked_mlp_transcoders,\n",
    ")\n",
    "from circuit_finder.patching.indirect_leap import (\n",
    "    preprocess_attn_saes,\n",
    "    IndirectLEAP,\n",
    "    LEAPConfig,\n",
    ")\n",
    "from circuit_finder.core.types import Model\n",
    "from circuit_finder.metrics import batch_avg_answer_diff\n",
    "from circuit_finder.constants import ProjectDir\n",
    "from circuit_finder.patching.ablate import (\n",
    "    splice_model_with_saes_and_transcoders,\n",
    "    get_metric_with_ablation,\n",
    "    AblateType,\n",
    ")\n",
    "\n",
    "from circuit_finder.experiments.run_dataset_sweep import ALL_DATASETS\n",
    "\n",
    "batch_size = 8\n",
    "print(ALL_DATASETS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import ActivationCache\n",
    "from circuit_finder.patching.ablate import (\n",
    "    splice_model_with_saes_and_transcoders,\n",
    "    filter_sae_acts_and_errors,\n",
    ")\n",
    "\n",
    "\n",
    "def get_cache(train_loader):\n",
    "\n",
    "    n_tokens = 0\n",
    "    total_tokens = 100_000 # 100k tokens\n",
    "    # total_tokens = 100\n",
    "    print(f\"Total tokens: {total_tokens}\")\n",
    "\n",
    "    # A bit of a hack, run once to get cache shapes\n",
    "    with splice_model_with_saes_and_transcoders(model, transcoders, saes):\n",
    "        _, dummy_cache = model.run_with_cache(\n",
    "            \"Hello World\", names_filter=filter_sae_acts_and_errors\n",
    "        )\n",
    "\n",
    "\n",
    "    zero_cache_dict = {\n",
    "        hook_name: torch.zeros_like(act.sum(1).squeeze(0))\n",
    "        for hook_name, act in dummy_cache.items()\n",
    "    }\n",
    "    # zero_cache = ActivationCache(zero_cache_dict, model)\n",
    "\n",
    "    # Run the model\n",
    "    with splice_model_with_saes_and_transcoders(model, transcoders, saes):\n",
    "        for batch in train_loader:\n",
    "            tokens = batch.clean\n",
    "            _, cache = model.run_with_cache(tokens, names_filter=filter_sae_acts_and_errors)\n",
    "\n",
    "            n_tokens += tokens.shape[1] * tokens.shape[0]\n",
    "            for hook_name, act in cache.items():\n",
    "                zero_cache_dict[hook_name] += act.sum(1).sum(0)\n",
    "\n",
    "            if n_tokens >= total_tokens:\n",
    "                break\n",
    "\n",
    "    print(n_tokens)\n",
    "\n",
    "    # Average the cache\n",
    "    for hook_name, act in zero_cache_dict.items():\n",
    "        zero_cache_dict[hook_name] /= n_tokens\n",
    "\n",
    "    zero_cache = ActivationCache(zero_cache_dict, model)\n",
    "    return zero_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing datasets/greaterthan_gpt2-small_prompts.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 100000\n",
      "1408\n",
      "Processing datasets/ioi/ioi_ABBA_template_0_prompts.json\n",
      "Total tokens: 100000\n",
      "2048\n",
      "Processing datasets/ioi/ioi_ABBA_template_1_prompts.json\n",
      "Total tokens: 100000\n",
      "2560\n",
      "Processing datasets/ioi/ioi_BABA_template_0_prompts.json\n",
      "Total tokens: 100000\n",
      "2048\n",
      "Processing datasets/ioi/ioi_BABA_template_1_prompts.json\n",
      "Total tokens: 100000\n",
      "2560\n"
     ]
    }
   ],
   "source": [
    "for dataset_path in ALL_DATASETS:\n",
    "    print(\"Processing\", dataset_path)\n",
    "    train_loader, _ = load_datasets_from_json(\n",
    "        model,\n",
    "        ProjectDir / dataset_path,\n",
    "        device=torch.device(\"cuda\"),\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    cache = get_cache(train_loader)\n",
    "    with open(f\"{pathlib.Path(dataset_path).stem}_mean_acts.pkl\", \"wb\") as file:\n",
    "        pickle.dump(zero_cache, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cache\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(f\"{pathlib.Path(dataset_path).stem}_mean_acts.pkl\", \"wb\") as file:\n",
    "    pickle.dump(zero_cache, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marks et al Datasets\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
