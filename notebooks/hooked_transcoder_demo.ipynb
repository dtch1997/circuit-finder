{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import transformer_lens as tl\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "from circuit_finder.core.hooked_sae import HookedSAE\n",
    "from circuit_finder.core.hooked_sae_config import HookedSAEConfig\n",
    "from typing import Union, Dict\n",
    "from dataclasses import dataclass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/circuit-finder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from circuit_finder.pretrained import load_model\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define HookedTranscoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "# flake8: noqa\n",
    "from __future__ import annotations\n",
    "\n",
    "import pprint\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformer_lens import utils\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HookedTranscoderConfig:\n",
    "    \"\"\"\n",
    "    Configuration class to store the configuration of a HookedSAE model.\n",
    "\n",
    "    Args:\n",
    "        d_sae (int): The size of the dictionary.\n",
    "        d_in (int): The dimension of the input activations.\n",
    "        hook_name (str): The hook name of the activation the SAE was trained on (eg. blocks.0.attn.hook_z)\n",
    "        use_error_term (bool): Whether to use the error term in the loss function. Defaults to False.\n",
    "        dtype (torch.dtype, *optional*): The SAE's dtype. Defaults to torch.float32.\n",
    "        seed (int, *optional*): The seed to use for the SAE.\n",
    "            Used to set sources of randomness (Python, PyTorch and\n",
    "            NumPy) and to initialize weights. Defaults to None. We recommend setting a seed, so your experiments are reproducible.\n",
    "        device(str): The device to use for the SAE. Defaults to 'cuda' if\n",
    "            available, else 'cpu'.\n",
    "    \"\"\"\n",
    "\n",
    "    d_sae: int\n",
    "    d_in: int\n",
    "    d_out: int\n",
    "    hook_name: str\n",
    "    hook_name_out: str\n",
    "    use_error_term: bool = False\n",
    "    dtype: torch.dtype = torch.float32\n",
    "    seed: Optional[int] = None\n",
    "    device: Optional[str] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.seed is not None:\n",
    "            self.set_seed_everywhere(self.seed)\n",
    "\n",
    "        if self.device is None:\n",
    "            self.device = utils.get_device()\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, config_dict: Dict[str, Any]) -> HookedTranscoderConfig:\n",
    "        \"\"\"\n",
    "        Instantiates a `HookedSAEConfig` from a Python dictionary of\n",
    "        parameters.\n",
    "        \"\"\"\n",
    "        return cls(**config_dict)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return self.__dict__\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"HookedSAEConfig:\\n\" + pprint.pformat(self.to_dict())\n",
    "\n",
    "    def set_seed_everywhere(self, seed: int):\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from jaxtyping import Float\n",
    "import transformer_lens as tl\n",
    "\n",
    "from transformer_lens.hook_points import HookPoint, HookedRootModule\n",
    "\n",
    "\n",
    "class HookedTranscoder(HookedRootModule):\n",
    "    \"\"\"Hooked Transcoder\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: Union[HookedTranscoderConfig, Dict]):\n",
    "        super().__init__()\n",
    "        if isinstance(cfg, Dict):\n",
    "            cfg = HookedTranscoderConfig(**cfg)\n",
    "        elif isinstance(cfg, str):\n",
    "            raise ValueError(\n",
    "                \"Please pass in a config dictionary or HookedSAEConfig object.\"\n",
    "            )\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.W_enc = nn.Parameter(\n",
    "            torch.nn.init.kaiming_uniform_(\n",
    "                torch.empty(self.cfg.d_in, self.cfg.d_sae, dtype=self.cfg.dtype)\n",
    "            )\n",
    "        )\n",
    "        self.W_dec = nn.Parameter(\n",
    "            torch.nn.init.kaiming_uniform_(\n",
    "                torch.empty(self.cfg.d_sae, self.cfg.d_in, dtype=self.cfg.dtype)\n",
    "            )\n",
    "        )\n",
    "        self.b_enc = nn.Parameter(torch.zeros(self.cfg.d_sae, dtype=self.cfg.dtype))\n",
    "        self.b_dec = nn.Parameter(torch.zeros(self.cfg.d_in, dtype=self.cfg.dtype))\n",
    "\n",
    "        self.hook_sae_input = HookPoint()\n",
    "        self.hook_sae_acts_pre = HookPoint()\n",
    "        self.hook_sae_acts_post = HookPoint()\n",
    "        self.hook_sae_recons = HookPoint()\n",
    "\n",
    "        self.to(self.cfg.device)\n",
    "        self.setup()\n",
    "\n",
    "    def maybe_reshape_input(\n",
    "        self,\n",
    "        input: Float[torch.Tensor, \"... d_input\"],\n",
    "        apply_hooks: bool = True,\n",
    "    ) -> Float[torch.Tensor, \"... d_in\"]:\n",
    "        \"\"\"\n",
    "        Reshape the input to have correct dim.\n",
    "        No-op for standard SAEs, but useful for hook_z SAEs.\n",
    "        \"\"\"\n",
    "        if apply_hooks:\n",
    "            self.hook_sae_input(input)\n",
    "\n",
    "        if input.shape[-1] == self.cfg.d_in:\n",
    "            x = input\n",
    "        else:\n",
    "            # Assume this this is an attention output (hook_z) SAE\n",
    "            assert self.cfg.hook_name.endswith(\n",
    "                \"_z\"\n",
    "            ), f\"You passed in an input shape {input.shape} does not match SAE input size {self.cfg.d_in} for hook_name {self.cfg.hook_name}. This is only supported for attn output (hook_z) SAEs.\"\n",
    "            x = einops.rearrange(input, \"... n_heads d_head -> ... (n_heads d_head)\")\n",
    "        assert (\n",
    "            x.shape[-1] == self.cfg.d_in\n",
    "        ), f\"Input shape {x.shape} does not match SAE input size {self.cfg.d_in}\"\n",
    "\n",
    "        return x\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        x: Float[torch.Tensor, \"... d_in\"],\n",
    "        apply_hooks: bool = True,\n",
    "    ) -> Float[torch.Tensor, \"... d_sae\"]:\n",
    "        \"\"\"SAE Encoder.\n",
    "\n",
    "        Args:\n",
    "            input: The input tensor of activations to the SAE. Shape [..., d_in].\n",
    "\n",
    "        Returns:\n",
    "            output: The encoded output tensor from the SAE. Shape [..., d_sae].\n",
    "        \"\"\"\n",
    "        # Subtract bias term\n",
    "        x_cent = x - self.b_dec\n",
    "\n",
    "        # SAE hidden layer pre-RELU  activation\n",
    "        sae_acts_pre = (\n",
    "            einops.einsum(x_cent, self.W_enc, \"... d_in, d_in d_sae -> ... d_sae\")\n",
    "            + self.b_enc  # [..., d_sae]\n",
    "        )\n",
    "        if apply_hooks:\n",
    "            sae_acts_pre = self.hook_sae_acts_pre(sae_acts_pre)\n",
    "\n",
    "        # SAE hidden layer post-RELU activation\n",
    "        sae_acts_post = F.relu(sae_acts_pre)  # [..., d_sae]\n",
    "        if apply_hooks:\n",
    "            sae_acts_post = self.hook_sae_acts_post(sae_acts_post)\n",
    "\n",
    "        return sae_acts_post\n",
    "\n",
    "    def decode(\n",
    "        self,\n",
    "        sae_acts_post: Float[torch.Tensor, \"... d_sae\"],\n",
    "        apply_hooks: bool = True,\n",
    "    ) -> Float[torch.Tensor, \"... d_in\"]:\n",
    "        x_reconstruct = (\n",
    "            einops.einsum(\n",
    "                sae_acts_post, self.W_dec, \"... d_sae, d_sae d_in -> ... d_in\"\n",
    "            )\n",
    "            + self.b_dec\n",
    "        )\n",
    "        if apply_hooks:\n",
    "            x_reconstruct = self.hook_sae_recons(x_reconstruct)\n",
    "        return x_reconstruct\n",
    "    \n",
    "    def maybe_reshape_output(\n",
    "        self,\n",
    "        input: Float[torch.Tensor, \"... d_input\"],\n",
    "        output: Float[torch.Tensor, \"... d_in\"],\n",
    "    ):\n",
    "        output = output.reshape(input.shape)\n",
    "        return output\n",
    "\n",
    "    def forward(\n",
    "        self, input: Float[torch.Tensor, \"... d_in\"]\n",
    "    ) -> Float[torch.Tensor, \"... d_in\"]:\n",
    "        \"\"\"SAE Forward Pass.\n",
    "\n",
    "        Args:\n",
    "            input: The input tensor of activations to the SAE. Shape [..., d_in].\n",
    "                Also supports hook_z activations of shape [..., n_heads, d_head], where n_heads * d_head = d_in, for attention output (hook_z) SAEs.\n",
    "\n",
    "        Returns:\n",
    "            output: The reconstructed output tensor from the SAE, with the error term optionally added. Same shape as input (eg [..., d_in])\n",
    "        \"\"\"\n",
    "        x = self.maybe_reshape_input(input)\n",
    "        sae_acts_post = self.encode(x)\n",
    "        x_reconstruct = self.decode(sae_acts_post)\n",
    "        return self.maybe_reshape_output(input, x_reconstruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Regular Transcoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce5a21ada5b43f3b63857834b8faca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from circuit_finder.pretrained import load_mlp_transcoders\n",
    "\n",
    "transcoder = load_mlp_transcoders([8])[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W_enc', 'b_enc', 'W_dec', 'b_dec', 'b_dec_out']\n"
     ]
    }
   ],
   "source": [
    "state_dict = transcoder.state_dict()\n",
    "print(list(state_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(transcoder.cfg.is_transcoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  1.1862,   1.5696,  -1.9906,  -1.2029,   2.2175,  -1.4098,  -1.4708,\n",
      "         -1.6490,  -2.0197,  -2.0229,   0.9865,  -1.8019,  -1.6369,  -2.2256,\n",
      "         -2.2842,   1.8605,   1.3689,  -1.9265,  -1.4797,   2.5357,  -1.8286,\n",
      "          1.0884,  -1.5102,   1.1545,  -1.8980,  -2.0404,  -1.3951,  -1.3454,\n",
      "          1.4757,   1.9774,  -1.9712,  -1.5186,   1.4978,  -1.9888,   0.9793,\n",
      "          1.5924,  -1.5015,  -1.0039,   1.7328,  -1.4052,   2.0603,   2.2594,\n",
      "          1.2320,  -2.3857,   1.2999,   1.3727,   1.0996,  -2.1323,   2.1212,\n",
      "         -1.1185,   1.8134,  -1.0758,  -1.4854,   1.8169,  -1.4866,   1.4962,\n",
      "          1.5304,  -0.9501,  -1.5599,  -1.7975,   1.6399,   1.6877,   1.8173,\n",
      "         -1.1781,  -4.6810,  -1.5628,   2.0368,   1.5156,   1.3177,   1.4963,\n",
      "          1.7548,   0.5594,  -0.7755,   1.2884,  -1.6782,   1.1857,   2.1568,\n",
      "         -2.1466,   1.2580,   1.8623,   2.0687,  -1.7108,  -1.6401,   1.3888,\n",
      "         -2.7691,   2.1310,  -1.8680,  -3.3717,   2.2827,   1.8747,  -1.8053,\n",
      "          1.3109,  -1.2763,  -1.7020,   1.2091,  -1.6898,  -2.0765,  -2.0752,\n",
      "         -1.7023,   2.1443,  -1.0929,   2.3768,  -1.4730,  -0.9105,  -1.3430,\n",
      "         -1.5172,  -1.8420,   1.4607,   1.8820,   1.0957,  -1.4175,  -2.1689,\n",
      "          1.2038,  -1.6439,   1.5685,  -1.8290,   1.3840,   2.0749,   1.7487,\n",
      "          1.8627,  -1.3516,   1.3826,  -1.6497,  -1.0906,   2.0661,  -0.8196,\n",
      "         -1.7572,   2.1426,  -1.2156,  -1.5028,   1.2622,   1.7093,  -1.7276,\n",
      "          1.5844,  -1.9632,   1.0784,  -1.1698,  -1.9105,   2.0976,  -1.9185,\n",
      "         -1.4808,   1.5521,   1.7983,   1.5732,  -1.4362,  -1.2335,  -2.1897,\n",
      "         -1.9735,   1.6523,  -1.9499,  -2.3766,  -0.8018,  -1.2022,  -1.3324,\n",
      "          1.4743,  -1.4036,  -1.4930,   1.4729,   1.1717,   1.6270,  -1.8226,\n",
      "         -1.7411,  -1.6088,  -1.6725,  -1.8819,  -2.1590,  -2.0173,   0.6144,\n",
      "         -1.9117,  -1.6784,   1.9339,  -1.5947,  -1.3813,  -1.8327,   1.5469,\n",
      "          1.7466,  -2.6949,   1.5565,  -1.7119,   1.5549,  -1.6332,  -1.2562,\n",
      "          1.7333,   1.5201,  -1.6721,  -1.8356,   1.2405,   1.6926,   1.3727,\n",
      "         -1.9896,   1.0286,  -2.2239,  -1.8138,   1.6551,  -1.7493,  -2.6261,\n",
      "         -1.5858,  -0.9370,   2.0919,  -1.3237,  -1.7332,   2.3992,  -1.6169,\n",
      "         -1.1490,   1.3382,  -1.7906,  -1.6235,   0.8017,   1.7386,   2.2358,\n",
      "         -2.1874,  -1.3780,  -1.8092,  -1.7477,  -2.1349,   1.1679,   1.2624,\n",
      "          1.0623,   1.1299,   1.3964,  -1.9391,   1.1418,   1.2971,  -0.4626,\n",
      "         -1.1607,  -1.5123,  -1.3846,   1.0751,  -1.1306,   1.9332,   1.2360,\n",
      "         -1.8277,  -1.0047,  -1.4524,  -1.5554,  -1.0894,   1.4941,  -1.2771,\n",
      "         -2.1009,  -1.3447,  -2.1938,  -1.9186,   1.6952,   2.0050,   1.5396,\n",
      "          1.3996,   1.6045,  -1.5862,   1.6694,  -1.0953,  -2.7994,  -2.0467,\n",
      "         -1.2095,  -1.5117,   1.6491,  -1.8872,  -1.3456,   0.5754,   2.7767,\n",
      "         -0.8542,   1.6781,   1.0031,  -1.7109,   1.6838,  -1.1726,  -1.8952,\n",
      "          0.6580,  -1.5226,   0.8195,  -0.8927,  -2.6390,   2.1960,   1.6872,\n",
      "          1.2293,  -0.8150,   1.0404,  -1.6245,  -1.5025,  -1.8897,  -0.8931,\n",
      "         -1.8616,   2.0611,  -0.8387,  -1.9953,  -1.7886,  -2.0746,   0.7358,\n",
      "          1.2882,  -0.7577,  -2.8885,   1.3884,   1.9475,   0.8087,   1.1380,\n",
      "          1.6585,   1.4909,  -1.1523,  -1.2761,  -1.0301,   1.5285,  -2.2370,\n",
      "          0.9744,  -1.3555,   1.3969,  -1.7546,  -1.5979,   1.1987,   1.1995,\n",
      "         -1.7566,   1.1886,   2.1219,   0.3238,  -1.8990,  -1.4164,  -0.2997,\n",
      "          1.3572,  -1.9432,  -1.6443,  -1.5768,   1.3810,  -2.0636,   1.9355,\n",
      "         -1.8059,   1.5277,   1.8222,  -2.1806,  -0.1903,   1.8439,  -0.9647,\n",
      "          1.7255,   1.8073,  -2.2111,   1.8120,   1.9179,  -1.7746,   1.0543,\n",
      "         -2.1592,   1.6094,   1.9746,  -1.8628,   0.8066,  -0.9744,  -1.4489,\n",
      "          1.2644,  -0.4444,  -2.2355,   1.3870,   2.0981,   1.6209,   2.0829,\n",
      "         -1.1804,   1.8182,   1.7262,   1.4438,   1.1051,  -2.4778,   0.9938,\n",
      "         -1.4919,   1.3177,   1.7218,   2.1099,  -1.0355,   1.9543,  -1.9763,\n",
      "         -1.8714,  -1.6343,  -1.7698,  -1.6523,  -1.5278,   1.5047,   1.1132,\n",
      "          0.8842,  -1.8732, -10.1460,  -1.7059,   0.8054,  -1.9830,   1.5892,\n",
      "          1.4822,   2.0276,  -1.5948,  -1.6749,   1.5674,  -1.7198,   1.3115,\n",
      "         -1.9994,  -1.6457,   1.1970,  -1.8241,  -0.7368,   1.4745,  -1.6208,\n",
      "          1.7403,   5.1591,   1.1149,   1.0525,  -1.6181,   1.9963,  -2.8913,\n",
      "         -2.0140,   2.0135,   2.1186,   1.8240,   1.1695,   1.7706,   1.8680,\n",
      "          1.4623,   0.9570,  -0.7539,  -2.3199,  -1.3809,   1.3010,  -2.1637,\n",
      "         -1.2613,  -1.8909,  -2.4220,   1.5306,   1.6519,   1.4749,  -1.2094,\n",
      "          1.4583,   2.1420,   1.5245,  -2.8291,   1.2220,  -1.8785,   1.0799,\n",
      "         -1.8690,  -1.2292,  -0.9358,   2.2647,   1.6420,  -1.9238,  -1.3176,\n",
      "         -1.9230,  -2.2551,  -1.2379,  -2.2933,  -1.6412,  -0.9690,   1.7062,\n",
      "          1.6066,   2.1428,   1.8397,   1.0643,  -1.2942,  -1.7669,  13.4893,\n",
      "          1.2908,   0.9234,   1.7631,   1.0362,  -1.8273,   1.2081,   1.2933,\n",
      "         -1.4260,   1.7737,   1.9660,  -2.1236,  -1.8818,   1.4920,  -2.1224,\n",
      "          2.1554,   1.0961,   1.7312,  -1.9389,   1.4440,   0.7970,   1.1455,\n",
      "         -1.8713,  -1.7096,   1.4072,  -1.5418,   1.0245,   0.7005,  -1.6033,\n",
      "         -2.0500,   1.6731,  -1.0272,  -2.5421,  -0.4983,   9.5209,   2.1932,\n",
      "         -1.6022,   0.9466,  -1.0898,   0.9502,  -1.5219,  -1.3479,   1.4430,\n",
      "         -1.7169,   1.4373,  -1.4993,  -1.5412,  -1.6229,  -1.8454,   2.5856,\n",
      "          1.7012,   1.7580,  -1.7331,   1.2464,  -1.7372,   1.6434,  -1.8866,\n",
      "         -1.5355,  -1.2141,   2.1551,   2.0600,  -1.6548,  -1.6292,   1.5121,\n",
      "         -1.8515,   1.9513,  -1.7424,   1.7693,   1.6607,  -1.4959,  -1.5686,\n",
      "         -1.4227,   1.9159,  -1.0295,   1.3319,  -1.8225,   0.8605,  -1.3973,\n",
      "          1.7146,   0.2748,  -2.0766,  -1.7578,  -1.3279,  -1.6653,  -1.6786,\n",
      "          1.5178,   1.1981,   1.7565,   1.2776,  -1.7127,  -1.7591,  -1.9839,\n",
      "          0.6868,   2.0483,  -1.1408,   1.6711,   1.8873,   1.1597,  -1.5430,\n",
      "          0.9397,  -1.7302,  -1.7735,  -1.1304,   1.4674,   1.6141,   1.4521,\n",
      "         -1.8439,   0.1695,   1.4475,  -1.2146,  -1.4663,  -1.9051,  -1.7489,\n",
      "         -1.5954,  -1.9749,   2.1142,   1.8338,  -1.5099,  -2.2768,  -2.6024,\n",
      "          1.1211,  -1.6411,  -1.9245,  -2.5265,   1.7450,  -0.8068,   1.3507,\n",
      "          1.8016,  -1.2199,   0.7254,   1.5378,   0.4692,   1.7285,  -0.4955,\n",
      "         -1.7589,  -2.0887,  -1.4349,   1.3877,  -1.3107,  -1.4167,  -0.8669,\n",
      "         -1.2891,  -2.3363,   1.7415,  -1.6511,  -1.5836,   1.6595,  -0.5033,\n",
      "         -1.8079,   1.3156,   1.8584,  -1.9732,   1.5361,  -1.9298,  -1.8470,\n",
      "          1.8956,  -2.0041,  -1.9238,  -1.4833,  -1.8188,   1.7353,   2.2662,\n",
      "         -1.5081,   1.5347,   1.7980,   1.8805,  -1.7895,   0.5048,  -1.6160,\n",
      "         -0.6627,   0.8723,   1.7146,  -0.8495,  -2.0294,   1.7694,   1.5569,\n",
      "          1.5696,   1.1692,   1.4559,   0.6400,   0.4853,  -1.7410,  -1.3952,\n",
      "          1.5241,  -1.4723,   1.7810,   1.8224,   1.3627,   1.1522,   1.2050,\n",
      "         -1.8779,   1.2733,   2.2287,  -0.9390,   1.5218,  -2.2275,   1.9319,\n",
      "          1.6584,   1.1448,  -1.3627,   1.3243,  -1.6451,  -1.8218,   1.8200,\n",
      "          1.6672,   1.2386,   1.1071,  -0.7957,  -1.3861,  -1.4268,  -1.1062,\n",
      "          2.2575,  -1.5995,  -2.0879,  -2.3657,   1.7198,  -1.7277,  -0.8128,\n",
      "         -1.3234,   1.5414,   0.6429,   1.2803,  -2.1206,  -1.9377,  -1.5177,\n",
      "          1.6821,   1.6600,  -1.4769,   1.5129,  -1.6838,   1.4905,  -1.6456,\n",
      "         -1.1824,   2.3522,  -2.1465,   1.9801,   1.6867,  -1.5320,  -2.2480,\n",
      "          1.0425,  -1.1384,   2.1362,  -0.8923,  -2.1348,  -1.8096,  -2.0330,\n",
      "          1.1206,  -2.0091,  -1.2227,   1.8565,   1.6048,  -1.8946,  -2.0054,\n",
      "         -1.7664,  -1.9106,   2.2141,   1.2170,   1.0962,  -1.3899,  -1.0852,\n",
      "         -1.8233,   0.7642,   2.1921,  -0.4618,   2.0822,   1.9128,   0.9422,\n",
      "          1.5129,   1.8154,  -1.1329,  -1.3421,  -2.1455,   1.7725,   2.2282,\n",
      "          1.8737,   0.7387,  -1.3180,   1.2838,  -1.1533,   1.9737,   1.4401,\n",
      "         -2.6842,  -1.2534,   1.5279,   1.1032,   1.1203,  -2.4098,   1.5062,\n",
      "         -1.9612,   1.6277,   1.8357,  -1.9027,  -1.6912,   0.7782,  -1.1805,\n",
      "         -2.4519,  -1.7429,  -0.8195,  -2.5943,   0.9560,  -1.8623,   2.1121,\n",
      "          1.9863,  -1.5646,   1.0463,  -1.1984,  -1.3467,  -1.3388,  -1.5867,\n",
      "          0.5745,  -1.3158,   1.0473,   1.7308,  -1.2877,  -1.2271,   2.2544,\n",
      "         -1.8512,  -1.3496,   1.5827,  -1.5183,  -1.8652], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(state_dict[\"b_dec\"].shape)\n",
    "print(state_dict[\"b_dec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n",
      "tensor([-1.2672e-02,  2.3626e-02, -1.3722e-01, -1.0775e-01,  8.1926e-02,\n",
      "         6.4808e-02,  2.6040e-01, -1.4005e-01, -4.7093e-02,  4.6066e-02,\n",
      "         1.0110e-01, -6.2927e-02, -1.5965e-01, -5.0537e-02, -2.8574e-02,\n",
      "         4.5400e-02, -1.0613e-01, -1.2895e-01,  5.0888e-02,  2.0695e-02,\n",
      "        -5.6016e-02, -3.0756e-02,  7.1454e-02, -1.1798e-01, -2.7476e-02,\n",
      "         8.0708e-03,  5.1458e-03, -1.0945e-01,  2.1156e-02, -6.4942e-02,\n",
      "         1.2061e-01,  5.8028e-02,  6.7034e-02,  6.0527e-02,  8.2463e-02,\n",
      "        -9.5162e-02,  4.5119e-01,  7.9933e-02, -6.0591e-02,  8.0231e-02,\n",
      "        -3.4440e-03,  8.6729e-02,  1.1527e-01, -5.9831e-02,  1.5111e-02,\n",
      "         1.6710e-01, -7.2933e-02,  1.1341e-01,  3.1571e-02,  4.7865e-02,\n",
      "        -1.0125e-01,  7.3603e-02,  2.8579e-02,  1.0483e-01,  9.6484e-02,\n",
      "         1.1278e-01,  1.8236e-01, -5.9445e-02,  3.0726e-02,  7.1907e-03,\n",
      "         2.6851e-02, -1.2508e-02,  1.1155e-01,  1.3431e-01, -6.2534e-02,\n",
      "        -8.6121e-02,  2.9323e-01,  4.3801e-03, -1.0330e-01,  1.1480e-02,\n",
      "         1.0512e-01,  6.6064e-02, -9.0331e-02,  1.7504e-02,  4.5555e-02,\n",
      "        -8.3929e-02, -2.0215e-02, -3.9352e-02, -1.4561e-02, -1.4071e-01,\n",
      "        -1.6326e-01, -9.2043e-02, -2.8400e-02,  2.5710e-02, -2.1654e-01,\n",
      "         9.6395e-03, -1.2446e-01, -9.1078e-03,  1.1859e-01, -7.3774e-02,\n",
      "         1.9797e-01, -4.9325e-02, -1.5266e-01,  7.9161e-02,  9.5230e-02,\n",
      "        -6.1189e-02, -4.5864e-02, -3.8277e-02, -1.9247e-01, -1.9493e-01,\n",
      "        -1.8296e-02, -4.5832e-02,  1.5284e-01, -7.8813e-02, -9.1278e-02,\n",
      "        -1.5813e-02, -1.5763e-01,  2.6801e-01,  1.2607e-01, -7.3311e-02,\n",
      "        -2.3722e-03, -1.5361e-01, -1.4700e-01, -2.2834e-01, -8.0602e-03,\n",
      "        -7.2461e-02, -2.0276e-01, -3.2759e-02, -9.5605e-02, -1.0535e-01,\n",
      "        -1.4557e-01, -2.2994e-02,  8.2276e-03,  6.9612e-02,  4.3349e-03,\n",
      "        -1.2908e-01,  6.1100e-02,  1.7372e-02, -1.2988e-01, -8.6192e-02,\n",
      "        -4.7916e-02, -3.0555e-02, -3.7828e-02,  5.2234e-02, -1.1093e-01,\n",
      "        -4.1556e-03, -2.5260e-02, -4.3945e-02,  9.2235e-02,  1.5959e-01,\n",
      "         4.1697e-02,  1.0227e-01, -3.7269e-03,  7.4291e-02,  1.7369e-01,\n",
      "         9.0067e-02,  1.2035e-01, -3.6445e-02,  8.3512e-02,  6.5067e-02,\n",
      "        -9.2677e-02,  9.3143e-02, -7.2573e-02, -7.5418e-02,  6.6331e-02,\n",
      "         8.4001e-02, -2.3754e-02, -3.4934e-02,  1.4356e-02,  6.7041e-02,\n",
      "        -2.6244e-02, -3.1142e-02,  1.2267e-01, -1.3544e-01, -4.0142e-02,\n",
      "        -2.8201e-01,  1.6791e-01, -1.3175e-01,  4.3296e-02, -5.4109e-02,\n",
      "         4.5997e-02,  1.3037e-02, -5.8901e-02,  3.8560e-02, -1.7382e-02,\n",
      "         1.1482e-01, -2.4748e-01, -6.5944e-02,  8.1745e-03,  4.7130e-02,\n",
      "        -8.2940e-02,  2.0329e-01, -4.8109e-02, -4.5584e-02, -9.2674e-02,\n",
      "        -1.0596e-02,  1.1132e-02,  2.5536e-02,  5.2050e-02,  7.6593e-02,\n",
      "         5.3881e-03, -1.2087e-01, -1.7349e-01,  1.2149e-01, -1.0275e-01,\n",
      "         4.7850e-02,  8.2687e-02, -4.3260e-02, -2.2741e-01, -2.8607e-02,\n",
      "        -3.4678e-02,  1.1453e-01, -1.0006e-01, -1.4208e-01,  9.3721e-02,\n",
      "        -5.3864e-02, -8.2891e-02, -4.8947e-02, -8.8786e-02,  1.1931e-01,\n",
      "         5.8454e-02, -4.0631e-02, -1.0330e-01, -1.5467e-01, -1.2194e-01,\n",
      "         3.0506e-02,  5.8474e-02, -1.5391e-03, -9.6036e-02, -1.0107e-01,\n",
      "        -9.0418e-03,  7.0369e-03,  1.0308e-01, -4.8710e-02,  1.1911e-01,\n",
      "        -2.5959e-02, -3.0204e-02, -5.2339e-02, -4.7369e-02,  7.8004e-02,\n",
      "         1.2328e-02,  4.8602e-02,  9.7828e-02,  6.5951e-02, -4.7032e-02,\n",
      "        -8.2180e-02, -4.2983e-02,  1.7990e-02, -7.3003e-02,  1.5159e-02,\n",
      "        -1.5299e-01,  4.0252e-02,  3.4591e-02,  1.0463e-01,  9.5880e-02,\n",
      "         4.6936e-02,  1.6510e-02, -1.6618e-01, -1.2753e-02, -8.8464e-02,\n",
      "        -9.1854e-02, -1.2396e-01,  1.3303e-01, -2.4430e-02, -1.5180e-01,\n",
      "         4.6940e-02,  5.6402e-02,  3.4832e-02,  4.9812e-02, -9.3343e-02,\n",
      "         7.0288e-02, -1.6451e-01,  4.4818e-02,  1.4503e-01, -1.1750e-01,\n",
      "        -3.3915e-02, -7.6242e-02,  6.2567e-02, -4.8223e-03, -3.4960e-02,\n",
      "         1.1693e-01, -1.1006e-02,  7.4537e-02,  2.0903e-01,  4.6220e-02,\n",
      "         8.5115e-02, -5.6726e-03, -5.1129e-02,  9.4207e-03, -1.4323e-02,\n",
      "         3.1373e-02,  3.4114e-02,  1.1636e-01, -1.7237e-01, -5.6161e-02,\n",
      "        -7.6869e-02,  7.8362e-03, -1.2760e-02,  7.0820e-02, -1.5017e-01,\n",
      "        -5.3691e-02, -5.8620e-02, -1.3871e-01, -3.7160e-02,  4.6390e-04,\n",
      "        -2.2132e-01, -8.6586e-02, -9.5822e-02,  1.0005e-01,  2.4645e-03,\n",
      "        -2.2186e-01,  9.1962e-02,  8.3109e-02, -7.6087e-02,  4.5349e-02,\n",
      "         2.1801e-01, -1.3593e-01, -5.0691e-02, -1.4156e-01, -1.0803e-01,\n",
      "         2.1316e-01, -1.5483e-02, -1.9732e-01, -1.6442e-01, -1.3501e+00,\n",
      "        -6.1926e-02, -2.4090e-01,  5.4754e-02, -2.3722e-02,  1.4274e-01,\n",
      "        -6.4199e-02,  3.4257e-02, -3.2704e-03,  5.5840e-02,  1.7520e-01,\n",
      "         8.7357e-02,  6.3842e-02,  2.7363e-02, -3.6199e-02, -1.1501e-01,\n",
      "        -1.1930e-02,  3.4201e-02,  1.9261e-01,  1.8239e-03,  3.7947e-02,\n",
      "        -7.8946e-03,  2.0086e-01, -3.5184e-02,  6.9672e-02,  4.4045e-02,\n",
      "         1.2677e-01,  8.7021e-03,  9.5608e-02,  4.6333e-02, -6.6447e-02,\n",
      "         7.1518e-02,  1.0723e-01, -6.8629e-02,  8.2291e-02, -7.6869e-02,\n",
      "        -1.4203e-01,  1.1529e-01,  5.9088e-02,  1.5053e-01,  1.1373e-01,\n",
      "        -1.3907e-01,  1.5034e-01, -2.0750e-01, -2.9346e-02, -9.1176e-02,\n",
      "         1.1783e-01, -1.2039e-01,  4.6629e-01, -1.5040e-01, -6.5499e-02,\n",
      "        -4.0761e-01, -2.8985e-02,  2.5189e-02,  1.0310e-01, -5.4567e-02,\n",
      "         2.7805e-03,  1.0081e-01, -1.0628e-01, -3.3063e+00, -5.1308e-01,\n",
      "         5.5898e-02,  7.5801e-02,  9.2385e-02, -9.3076e-02, -5.7181e-02,\n",
      "        -4.5791e-02,  1.6963e-02, -8.6434e-03, -4.2952e-02, -5.9703e-03,\n",
      "         1.6075e-01,  7.4530e-02, -5.4559e-03, -3.9304e-02, -1.1215e-01,\n",
      "         7.3677e-02, -2.2040e-04,  1.5215e-01,  8.9228e-02, -1.0831e-01,\n",
      "        -2.0463e-02, -2.0993e-01,  3.8759e-02, -1.7399e-01,  5.5767e-03,\n",
      "        -2.5811e-02,  4.6437e-02, -1.2639e-01,  6.6566e-03,  1.0992e-02,\n",
      "        -7.9354e-02,  3.3377e-02, -3.4842e-02,  5.7591e-01,  5.0896e-02,\n",
      "         7.7418e-02, -9.0106e-02, -1.1872e-01, -2.5393e-02, -2.9157e-01,\n",
      "        -2.4352e-02,  2.2019e-02,  9.5031e-02, -6.9030e-02, -1.2203e-01,\n",
      "        -4.3921e-02, -5.9556e-02, -8.0541e-02, -4.1678e-02, -4.5786e-02,\n",
      "        -1.3901e-01, -7.6819e-02, -1.3514e-01,  1.0434e-01,  5.0822e-02,\n",
      "         8.8375e-01,  3.9455e-02,  1.2830e-01,  2.1711e-03, -1.2532e-01,\n",
      "        -1.4424e-02,  5.0051e-02,  6.6743e-02, -1.3636e-01,  7.5548e-02,\n",
      "        -8.9118e-02,  1.8837e-02,  3.2085e-01, -2.4813e-02,  1.6466e-01,\n",
      "         5.2996e-02, -1.3824e-01,  3.2447e+00, -1.2466e-01,  3.7126e-03,\n",
      "        -1.7728e-01,  4.2746e-02, -2.6336e-02,  9.1938e-02, -2.6453e-02,\n",
      "        -1.9823e-01, -1.0483e-01, -9.7100e-02,  1.0089e-01, -1.6239e-01,\n",
      "        -5.2085e-02,  9.9635e-02, -1.7754e-01, -1.4078e-01, -8.6982e-02,\n",
      "         1.3191e-02,  3.1111e-03, -1.0429e-01,  5.1088e-02,  9.0677e-02,\n",
      "        -1.4781e-01, -8.7566e-02, -7.4795e-02,  2.0798e-02,  1.4157e-01,\n",
      "         1.5232e-02, -2.3124e-02, -2.0260e-01, -1.1696e-01, -1.4057e-01,\n",
      "        -3.1560e-02,  5.4999e+00, -5.6139e-04,  6.4493e-02, -4.5117e-02,\n",
      "        -7.5014e-02,  2.1886e-02,  7.7582e-02,  6.4704e-02, -2.5877e-02,\n",
      "         1.8999e-01,  7.9906e-02,  3.0371e-02, -1.8259e-01, -1.3356e-01,\n",
      "        -1.7286e-01,  1.1270e+00, -3.4466e-01,  1.1427e-01, -7.6897e-02,\n",
      "        -5.2296e-03,  2.2420e-02,  2.8077e-03, -2.0937e-01, -8.3965e-02,\n",
      "        -1.8119e-01,  4.7390e-02,  1.3330e-01, -1.1826e-02, -5.2839e-02,\n",
      "         1.3537e-01,  1.3613e-01, -2.9718e-02, -8.5766e-02,  4.9825e-02,\n",
      "        -1.2658e-01,  9.3608e-02, -1.7993e-01,  1.5347e-02,  1.7228e-02,\n",
      "        -4.7618e-02, -2.0338e-01, -9.3996e-02,  9.7164e-02,  9.4420e-02,\n",
      "         2.2344e-02, -1.2228e-01, -1.5584e-01, -2.5847e-02, -3.9300e-02,\n",
      "         1.2959e-02,  1.3491e-01,  1.3167e-02,  2.4413e-03, -1.0840e-01,\n",
      "        -5.7609e-02, -1.0332e-01, -2.2049e-02,  1.3023e-02, -1.8118e-01,\n",
      "        -1.2031e-02, -5.7113e-03,  1.8768e-01,  1.0175e-01, -3.9195e-02,\n",
      "        -2.2516e-01, -6.1035e-02, -1.3730e-01, -3.3381e-02, -3.4056e-02,\n",
      "         8.3305e-02, -4.4345e-02,  7.4610e-02, -1.4407e-02,  9.3293e-02,\n",
      "         1.1915e-01, -1.2784e-01, -4.9231e-02, -8.5836e-02, -1.1472e-01,\n",
      "        -1.9696e-01,  7.2062e-02,  1.8465e-02,  5.7553e-03, -4.8892e-02,\n",
      "        -2.3922e-02,  1.2829e-02, -1.8968e-01, -2.7686e-02,  1.8536e-02,\n",
      "        -1.5443e-01, -7.7077e-02,  2.7316e-02,  3.2945e-02,  5.8950e-03,\n",
      "         3.4340e-02,  9.6937e-02,  2.8852e-02, -2.2168e-02,  2.7260e-01,\n",
      "         2.1969e-01, -2.0474e-03, -4.2179e-02, -9.4133e-03, -6.5413e-02,\n",
      "         1.4460e-01,  2.0588e-02,  3.4655e-02,  1.4017e-01, -1.1046e-01,\n",
      "         1.2893e-01, -5.9935e-02, -1.0867e-01, -1.9483e-02,  6.2776e-02,\n",
      "         1.4642e-01, -1.3361e-02,  1.8613e-01,  1.1198e-03,  1.0259e-01,\n",
      "        -1.0881e-01, -3.8721e-02,  2.1018e-02, -1.4121e-01, -6.5716e-02,\n",
      "        -3.6092e-02, -8.1182e-02,  1.5352e-01,  2.5812e-01,  1.0271e-01,\n",
      "         5.7960e-02,  2.8295e-02,  1.6678e-01, -6.9078e-02,  4.4645e-04,\n",
      "         2.7859e-02, -2.9412e-02, -1.2587e-01,  1.6200e-01, -1.2852e-01,\n",
      "        -1.1942e-01, -2.7531e-02,  1.7503e-02,  1.7830e-01, -1.9713e-02,\n",
      "        -7.1447e-02, -9.5432e-03, -2.5917e-01,  4.9468e-02, -1.2570e-01,\n",
      "         1.0987e-01, -1.6847e-01, -7.0937e-02, -4.6133e-02,  2.8589e-02,\n",
      "        -5.8319e-02, -2.0586e-01, -7.0145e-02,  5.1890e-02, -7.1064e-02,\n",
      "         1.2918e-01,  1.7134e-02,  3.7579e-02, -1.0460e-02, -1.9717e-02,\n",
      "        -4.9602e-02, -7.9332e-02,  6.5892e-02, -4.2534e-02, -1.7050e-01,\n",
      "         7.8611e-02, -6.1354e-03,  2.0804e-01,  1.1181e-01, -5.7442e-03,\n",
      "        -1.6725e-01,  8.0712e-02,  6.7578e-02,  9.2726e-02, -2.2962e-02,\n",
      "        -4.0677e-02, -1.2054e-01,  1.0553e-01, -1.5147e-01, -7.3870e-02,\n",
      "         1.7954e-02,  7.6682e-02, -1.9002e-01, -4.6251e-02,  2.1517e-01,\n",
      "        -5.8149e-03, -2.0204e-02, -6.8868e-03,  3.3949e-02, -7.0428e-02,\n",
      "        -6.0397e-02, -1.3378e-02, -7.6023e-02, -8.3824e-03,  1.9554e-01,\n",
      "         1.2088e-02,  1.6612e-01, -4.9736e-02,  3.7575e-02,  8.9847e-02,\n",
      "         5.7032e-02, -1.3900e-02, -1.0796e-01,  1.6558e-01, -9.9607e-02,\n",
      "        -1.3410e-01, -1.7425e-02,  9.6690e-02,  6.3148e-02, -1.8836e-02,\n",
      "         1.5489e-02,  4.0287e-02, -5.6824e-02,  6.3605e-02,  5.7437e-02,\n",
      "         1.0187e-01, -1.7224e-01,  5.2933e-02, -1.7021e-01, -2.4946e-01,\n",
      "         1.0112e-02,  2.4536e-02,  7.0304e-02,  1.2996e-02, -3.4616e-02,\n",
      "        -1.0441e-01,  4.0099e-02, -1.3621e-01,  2.3797e-02,  1.0170e-01,\n",
      "        -8.7419e-02,  1.9461e-01, -2.5345e-04,  1.6951e-02, -9.8519e-02,\n",
      "         1.4623e-01, -1.2063e-02,  1.2546e-01,  6.3233e-02,  1.6746e-01,\n",
      "        -4.9878e-02,  1.1971e-01,  1.9637e-01, -3.0333e-02, -2.8667e-02,\n",
      "         1.1593e-01, -7.0142e-02, -2.4104e-01, -3.0923e-02, -1.3758e-01,\n",
      "        -8.2418e-02,  8.9903e-02, -1.9326e-03, -4.0951e-02,  6.1190e-02,\n",
      "         1.1014e-01, -1.1507e-01, -8.2135e-02,  9.2759e-02, -1.6850e-01,\n",
      "        -9.6211e-02, -6.5935e-03,  7.9021e-02,  5.4684e-02,  4.9995e-03,\n",
      "        -6.8450e-02,  1.2270e-01,  6.5198e-02, -1.2809e-01, -3.5562e-02,\n",
      "        -3.2576e-02,  2.3883e-01, -4.4838e-02, -9.9344e-02, -2.6358e-01,\n",
      "        -1.0393e-01,  5.0160e-02,  3.1743e-01,  3.8070e-02, -8.5789e-02,\n",
      "        -7.8174e-02,  1.3010e-01, -1.0816e-01], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(state_dict['b_dec_out'].shape)\n",
    "print(state_dict['b_dec_out'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test HookedTranscoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884f7ce02b4548c497ab8556b65beda1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer = 8\n",
    "transcoder = load_mlp_transcoders([layer])[layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a transcoder to a HookedTranscoder\n",
    "\n",
    "import torch\n",
    "\n",
    "from transcoders_slim.sae_training.config import LanguageModelSAERunnerConfig\n",
    "from transcoders_slim.sae_training.sparse_autoencoder import SparseAutoencoder\n",
    "\n",
    "\n",
    "def sl_sae_cfg_to_tl_sae_cfg(\n",
    "    resid_sae_cfg: LanguageModelSAERunnerConfig,\n",
    ") -> HookedTranscoderConfig:\n",
    "    new_cfg = {\n",
    "        \"d_sae\": resid_sae_cfg.d_sae,\n",
    "        \"d_in\": resid_sae_cfg.d_in,\n",
    "        \"d_out\": resid_sae_cfg.d_out,\n",
    "        \"hook_name\": resid_sae_cfg.hook_point,\n",
    "        \"hook_name_out\": resid_sae_cfg.out_hook_point,\n",
    "    }\n",
    "    return HookedTranscoderConfig.from_dict(new_cfg)\n",
    "\n",
    "\n",
    "def sl_sae_to_tl_sae(\n",
    "    sl_sae: SparseAutoencoder,\n",
    ") -> HookedTranscoder:\n",
    "    state_dict = sl_sae.state_dict()\n",
    "    # NOTE: b_dec is unused\n",
    "    del state_dict['b_dec']\n",
    "    state_dict['b_dec'] = state_dict['b_dec_out']\n",
    "    del state_dict['b_dec_out']\n",
    "\n",
    "    cfg = sl_sae_cfg_to_tl_sae_cfg(sl_sae.cfg)\n",
    "    tl_sae = HookedTranscoder(cfg)\n",
    "    tl_sae.load_state_dict(state_dict)\n",
    "    return tl_sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooked_transcoder = sl_sae_to_tl_sae(transcoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HookedTranscoderWrapper(HookedRootModule):\n",
    "    \"\"\"Wrapper around transcoder and the MLP it replaces\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transcoder: HookedTranscoder,\n",
    "        mlp: MLP,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.transcoder = transcoder\n",
    "        self.mlp = mlp\n",
    "\n",
    "        self.hook_sae_error = HookPoint()\n",
    "        self.hook_sae_output = HookPoint()\n",
    "        self.mlp.to(transcoder.cfg.device)\n",
    "        self.setup()\n",
    "\n",
    "    @property\n",
    "    def cfg(self):\n",
    "        return self.transcoder.cfg\n",
    "\n",
    "    def forward(self, x):\n",
    "        sae_output = self.transcoder(x)\n",
    "        if self.cfg.use_error_term:\n",
    "            with torch.no_grad():\n",
    "                clean_sae_out = self.transcoder(x)\n",
    "                clean_mlp_out = self.mlp(x)\n",
    "                sae_error = clean_mlp_out - clean_sae_out\n",
    "            sae_error.requires_grad = True\n",
    "            sae_error.retain_grad()\n",
    "            sae_error = self.hook_sae_error(sae_error)\n",
    "            sae_output += sae_error\n",
    "        return self.hook_sae_output(sae_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTranscoderWrapper(\n",
       "  (transcoder): HookedTranscoder(\n",
       "    (hook_sae_input): HookPoint()\n",
       "    (hook_sae_acts_pre): HookPoint()\n",
       "    (hook_sae_acts_post): HookPoint()\n",
       "    (hook_sae_recons): HookPoint()\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (hook_pre): HookPoint()\n",
       "    (hook_post): HookPoint()\n",
       "  )\n",
       "  (hook_sae_error): HookPoint()\n",
       "  (hook_sae_output): HookPoint()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HookedTranscoderWrapper(\n",
    "    hooked_transcoder, model.blocks[layer].mlp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- context manager for replacing MLP sublayers with transcoders ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformer_lens as tl\n",
    "from typing import Sequence\n",
    "from transcoders_slim.transcoder import Transcoder\n",
    "from circuit_finder.core.types import LayerIndex\n",
    "from transformer_lens.HookedSAETransformer import set_deep_attr\n",
    "\n",
    "MLP = nn.Module\n",
    "\n",
    "def get_layer_of_hook_name(hook_point):\n",
    "    return int(hook_point.split('.')[1])\n",
    "\n",
    "class TranscoderReplacementContext:\n",
    "    \"\"\"Context manager to replace MLP sublayers with transcoders\"\"\"\n",
    "\n",
    "    model: tl.HookedTransformer\n",
    "    transcoders: Sequence[HookedTranscoder]\n",
    "    layers: Sequence[LayerIndex]\n",
    "    original_mlps: Sequence[MLP]\n",
    "\n",
    "    def __init__(self, model: tl.HookedTransformer, transcoders: Sequence[HookedTranscoder]):\n",
    "        self.layers = [get_layer_of_hook_name(t.cfg.hook_name) for t in transcoders]\n",
    "        self.original_mlps = [model.blocks[layer].mlp for layer in self.layers]\n",
    "        self.transcoders = transcoders\n",
    "        self.model = model\n",
    "\n",
    "    def __enter__(self):\n",
    "        for layer, transcoder in zip(self.layers, self.transcoders):\n",
    "            mlp = self.model.blocks[layer].mlp\n",
    "            self.model.blocks[layer].mlp = HookedTranscoderWrapper(\n",
    "                transcoder, mlp\n",
    "            )\n",
    "            # Adds the hooks to the model\n",
    "            self.model.setup()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
    "        for layer, mlp in zip(self.layers, self.original_mlps):\n",
    "            self.model.blocks[layer].mlp = mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to end teest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994ab9232c264af18ea87433327836a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transcoders = load_mlp_transcoders()\n",
    "hooked_transcoders = {k: sl_sae_to_tl_sae(v) for k, v in transcoders.items()}\n",
    "for transcoder in hooked_transcoders.values():\n",
    "    transcoder.cfg.use_error_term = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/circuit-finder/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import transformer_lens as tl\n",
    "\n",
    "model = tl.HookedSAETransformer.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    device=\"cuda\",\n",
    "    fold_ln=True,\n",
    "    center_writing_weights=True,\n",
    "    center_unembed=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7746, device='cuda:0', grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "text = \"When John and Mary went to the shops, John gave a bottle to Mary\"\n",
    "\n",
    "orig_loss, _ = model.run_with_cache(text, return_type = \"loss\")\n",
    "print(orig_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7746, device='cuda:0', grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "with TranscoderReplacementContext(\n",
    "    model, list(hooked_transcoders.values())\n",
    "):\n",
    "    spliced_loss, cache = model.run_with_cache(text, return_type = \"loss\")\n",
    "    print(spliced_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed\n",
      "hook_pos_embed\n",
      "blocks.0.hook_resid_pre\n",
      "blocks.0.ln1.hook_scale\n",
      "blocks.0.ln1.hook_normalized\n",
      "blocks.0.attn.hook_q\n",
      "blocks.0.attn.hook_k\n",
      "blocks.0.attn.hook_v\n",
      "blocks.0.attn.hook_attn_scores\n",
      "blocks.0.attn.hook_pattern\n",
      "blocks.0.attn.hook_z\n",
      "blocks.0.hook_attn_out\n",
      "blocks.0.hook_resid_mid\n",
      "blocks.0.ln2.hook_scale\n",
      "blocks.0.ln2.hook_normalized\n",
      "blocks.0.mlp.transcoder.hook_sae_input\n",
      "blocks.0.mlp.transcoder.hook_sae_acts_pre\n",
      "blocks.0.mlp.transcoder.hook_sae_acts_post\n",
      "blocks.0.mlp.transcoder.hook_sae_recons\n",
      "blocks.0.mlp.mlp.hook_pre\n",
      "blocks.0.mlp.mlp.hook_post\n",
      "blocks.0.mlp.hook_sae_error\n",
      "blocks.0.mlp.hook_sae_output\n",
      "blocks.0.hook_mlp_out\n",
      "blocks.0.hook_resid_post\n",
      "blocks.1.hook_resid_pre\n",
      "blocks.1.ln1.hook_scale\n",
      "blocks.1.ln1.hook_normalized\n",
      "blocks.1.attn.hook_q\n",
      "blocks.1.attn.hook_k\n",
      "blocks.1.attn.hook_v\n",
      "blocks.1.attn.hook_attn_scores\n",
      "blocks.1.attn.hook_pattern\n",
      "blocks.1.attn.hook_z\n",
      "blocks.1.hook_attn_out\n",
      "blocks.1.hook_resid_mid\n",
      "blocks.1.ln2.hook_scale\n",
      "blocks.1.ln2.hook_normalized\n",
      "blocks.1.mlp.transcoder.hook_sae_input\n",
      "blocks.1.mlp.transcoder.hook_sae_acts_pre\n",
      "blocks.1.mlp.transcoder.hook_sae_acts_post\n",
      "blocks.1.mlp.transcoder.hook_sae_recons\n",
      "blocks.1.mlp.mlp.hook_pre\n",
      "blocks.1.mlp.mlp.hook_post\n",
      "blocks.1.mlp.hook_sae_error\n",
      "blocks.1.mlp.hook_sae_output\n",
      "blocks.1.hook_mlp_out\n",
      "blocks.1.hook_resid_post\n",
      "blocks.2.hook_resid_pre\n",
      "blocks.2.ln1.hook_scale\n",
      "blocks.2.ln1.hook_normalized\n",
      "blocks.2.attn.hook_q\n",
      "blocks.2.attn.hook_k\n",
      "blocks.2.attn.hook_v\n",
      "blocks.2.attn.hook_attn_scores\n",
      "blocks.2.attn.hook_pattern\n",
      "blocks.2.attn.hook_z\n",
      "blocks.2.hook_attn_out\n",
      "blocks.2.hook_resid_mid\n",
      "blocks.2.ln2.hook_scale\n",
      "blocks.2.ln2.hook_normalized\n",
      "blocks.2.mlp.transcoder.hook_sae_input\n",
      "blocks.2.mlp.transcoder.hook_sae_acts_pre\n",
      "blocks.2.mlp.transcoder.hook_sae_acts_post\n",
      "blocks.2.mlp.transcoder.hook_sae_recons\n",
      "blocks.2.mlp.mlp.hook_pre\n",
      "blocks.2.mlp.mlp.hook_post\n",
      "blocks.2.mlp.hook_sae_error\n",
      "blocks.2.mlp.hook_sae_output\n",
      "blocks.2.hook_mlp_out\n",
      "blocks.2.hook_resid_post\n",
      "blocks.3.hook_resid_pre\n",
      "blocks.3.ln1.hook_scale\n",
      "blocks.3.ln1.hook_normalized\n",
      "blocks.3.attn.hook_q\n",
      "blocks.3.attn.hook_k\n",
      "blocks.3.attn.hook_v\n",
      "blocks.3.attn.hook_attn_scores\n",
      "blocks.3.attn.hook_pattern\n",
      "blocks.3.attn.hook_z\n",
      "blocks.3.hook_attn_out\n",
      "blocks.3.hook_resid_mid\n",
      "blocks.3.ln2.hook_scale\n",
      "blocks.3.ln2.hook_normalized\n",
      "blocks.3.mlp.transcoder.hook_sae_input\n",
      "blocks.3.mlp.transcoder.hook_sae_acts_pre\n",
      "blocks.3.mlp.transcoder.hook_sae_acts_post\n",
      "blocks.3.mlp.transcoder.hook_sae_recons\n",
      "blocks.3.mlp.mlp.hook_pre\n",
      "blocks.3.mlp.mlp.hook_post\n",
      "blocks.3.mlp.hook_sae_error\n",
      "blocks.3.mlp.hook_sae_output\n",
      "blocks.3.hook_mlp_out\n",
      "blocks.3.hook_resid_post\n",
      "blocks.4.hook_resid_pre\n",
      "blocks.4.ln1.hook_scale\n",
      "blocks.4.ln1.hook_normalized\n",
      "blocks.4.attn.hook_q\n",
      "blocks.4.attn.hook_k\n",
      "blocks.4.attn.hook_v\n",
      "blocks.4.attn.hook_attn_scores\n",
      "blocks.4.attn.hook_pattern\n",
      "blocks.4.attn.hook_z\n",
      "blocks.4.hook_attn_out\n",
      "blocks.4.hook_resid_mid\n",
      "blocks.4.ln2.hook_scale\n",
      "blocks.4.ln2.hook_normalized\n",
      "blocks.4.mlp.transcoder.hook_sae_input\n",
      "blocks.4.mlp.transcoder.hook_sae_acts_pre\n",
      "blocks.4.mlp.transcoder.hook_sae_acts_post\n",
      "blocks.4.mlp.transcoder.hook_sae_recons\n",
      "blocks.4.mlp.mlp.hook_pre\n",
      "blocks.4.mlp.mlp.hook_post\n",
      "blocks.4.mlp.hook_sae_error\n",
      "blocks.4.mlp.hook_sae_output\n",
      "blocks.4.hook_mlp_out\n",
      "blocks.4.hook_resid_post\n",
      "blocks.5.hook_resid_pre\n",
      "blocks.5.ln1.hook_scale\n",
      "blocks.5.ln1.hook_normalized\n",
      "blocks.5.attn.hook_q\n",
      "blocks.5.attn.hook_k\n",
      "blocks.5.attn.hook_v\n",
      "blocks.5.attn.hook_attn_scores\n",
      "blocks.5.attn.hook_pattern\n",
      "blocks.5.attn.hook_z\n",
      "blocks.5.hook_attn_out\n",
      "blocks.5.hook_resid_mid\n",
      "blocks.5.ln2.hook_scale\n",
      "blocks.5.ln2.hook_normalized\n",
      "blocks.5.mlp.transcoder.hook_sae_input\n",
      "blocks.5.mlp.transcoder.hook_sae_acts_pre\n",
      "blocks.5.mlp.transcoder.hook_sae_acts_post\n",
      "blocks.5.mlp.transcoder.hook_sae_recons\n",
      "blocks.5.mlp.mlp.hook_pre\n",
      "blocks.5.mlp.mlp.hook_post\n",
      "blocks.5.mlp.hook_sae_error\n",
      "blocks.5.mlp.hook_sae_output\n",
      "blocks.5.hook_mlp_out\n",
      "blocks.5.hook_resid_post\n",
      "blocks.6.hook_resid_pre\n",
      "blocks.6.ln1.hook_scale\n",
      "blocks.6.ln1.hook_normalized\n",
      "blocks.6.attn.hook_q\n",
      "blocks.6.attn.hook_k\n",
      "blocks.6.attn.hook_v\n",
      "blocks.6.attn.hook_attn_scores\n",
      "blocks.6.attn.hook_pattern\n",
      "blocks.6.attn.hook_z\n",
      "blocks.6.hook_attn_out\n",
      "blocks.6.hook_resid_mid\n",
      "blocks.6.ln2.hook_scale\n",
      "blocks.6.ln2.hook_normalized\n",
      "blocks.6.mlp.transcoder.hook_sae_input\n",
      "blocks.6.mlp.transcoder.hook_sae_acts_pre\n",
      "blocks.6.mlp.transcoder.hook_sae_acts_post\n",
      "blocks.6.mlp.transcoder.hook_sae_recons\n",
      "blocks.6.mlp.mlp.hook_pre\n",
      "blocks.6.mlp.mlp.hook_post\n",
      "blocks.6.mlp.hook_sae_error\n",
      "blocks.6.mlp.hook_sae_output\n",
      "blocks.6.hook_mlp_out\n",
      "blocks.6.hook_resid_post\n",
      "blocks.7.hook_resid_pre\n",
      "blocks.7.ln1.hook_scale\n",
      "blocks.7.ln1.hook_normalized\n",
      "blocks.7.attn.hook_q\n",
      "blocks.7.attn.hook_k\n",
      "blocks.7.attn.hook_v\n",
      "blocks.7.attn.hook_attn_scores\n",
      "blocks.7.attn.hook_pattern\n",
      "blocks.7.attn.hook_z\n",
      "blocks.7.hook_attn_out\n",
      "blocks.7.hook_resid_mid\n",
      "blocks.7.ln2.hook_scale\n",
      "blocks.7.ln2.hook_normalized\n",
      "blocks.7.mlp.transcoder.hook_sae_input\n",
      "blocks.7.mlp.transcoder.hook_sae_acts_pre\n",
      "blocks.7.mlp.transcoder.hook_sae_acts_post\n",
      "blocks.7.mlp.transcoder.hook_sae_recons\n",
      "blocks.7.mlp.mlp.hook_pre\n",
      "blocks.7.mlp.mlp.hook_post\n",
      "blocks.7.mlp.hook_sae_error\n",
      "blocks.7.mlp.hook_sae_output\n",
      "blocks.7.hook_mlp_out\n",
      "blocks.7.hook_resid_post\n",
      "blocks.8.hook_resid_pre\n",
      "blocks.8.ln1.hook_scale\n",
      "blocks.8.ln1.hook_normalized\n",
      "blocks.8.attn.hook_q\n",
      "blocks.8.attn.hook_k\n",
      "blocks.8.attn.hook_v\n",
      "blocks.8.attn.hook_attn_scores\n",
      "blocks.8.attn.hook_pattern\n",
      "blocks.8.attn.hook_z\n",
      "blocks.8.hook_attn_out\n",
      "blocks.8.hook_resid_mid\n",
      "blocks.8.ln2.hook_scale\n",
      "blocks.8.ln2.hook_normalized\n",
      "blocks.8.mlp.transcoder.hook_sae_input\n",
      "blocks.8.mlp.transcoder.hook_sae_acts_pre\n",
      "blocks.8.mlp.transcoder.hook_sae_acts_post\n",
      "blocks.8.mlp.transcoder.hook_sae_recons\n",
      "blocks.8.mlp.mlp.hook_pre\n",
      "blocks.8.mlp.mlp.hook_post\n",
      "blocks.8.mlp.hook_sae_error\n",
      "blocks.8.mlp.hook_sae_output\n",
      "blocks.8.hook_mlp_out\n",
      "blocks.8.hook_resid_post\n",
      "blocks.9.hook_resid_pre\n",
      "blocks.9.ln1.hook_scale\n",
      "blocks.9.ln1.hook_normalized\n",
      "blocks.9.attn.hook_q\n",
      "blocks.9.attn.hook_k\n",
      "blocks.9.attn.hook_v\n",
      "blocks.9.attn.hook_attn_scores\n",
      "blocks.9.attn.hook_pattern\n",
      "blocks.9.attn.hook_z\n",
      "blocks.9.hook_attn_out\n",
      "blocks.9.hook_resid_mid\n",
      "blocks.9.ln2.hook_scale\n",
      "blocks.9.ln2.hook_normalized\n",
      "blocks.9.mlp.transcoder.hook_sae_input\n",
      "blocks.9.mlp.transcoder.hook_sae_acts_pre\n",
      "blocks.9.mlp.transcoder.hook_sae_acts_post\n",
      "blocks.9.mlp.transcoder.hook_sae_recons\n",
      "blocks.9.mlp.mlp.hook_pre\n",
      "blocks.9.mlp.mlp.hook_post\n",
      "blocks.9.mlp.hook_sae_error\n",
      "blocks.9.mlp.hook_sae_output\n",
      "blocks.9.hook_mlp_out\n",
      "blocks.9.hook_resid_post\n",
      "blocks.10.hook_resid_pre\n",
      "blocks.10.ln1.hook_scale\n",
      "blocks.10.ln1.hook_normalized\n",
      "blocks.10.attn.hook_q\n",
      "blocks.10.attn.hook_k\n",
      "blocks.10.attn.hook_v\n",
      "blocks.10.attn.hook_attn_scores\n",
      "blocks.10.attn.hook_pattern\n",
      "blocks.10.attn.hook_z\n",
      "blocks.10.hook_attn_out\n",
      "blocks.10.hook_resid_mid\n",
      "blocks.10.ln2.hook_scale\n",
      "blocks.10.ln2.hook_normalized\n",
      "blocks.10.mlp.transcoder.hook_sae_input\n",
      "blocks.10.mlp.transcoder.hook_sae_acts_pre\n",
      "blocks.10.mlp.transcoder.hook_sae_acts_post\n",
      "blocks.10.mlp.transcoder.hook_sae_recons\n",
      "blocks.10.mlp.mlp.hook_pre\n",
      "blocks.10.mlp.mlp.hook_post\n",
      "blocks.10.mlp.hook_sae_error\n",
      "blocks.10.mlp.hook_sae_output\n",
      "blocks.10.hook_mlp_out\n",
      "blocks.10.hook_resid_post\n",
      "blocks.11.hook_resid_pre\n",
      "blocks.11.ln1.hook_scale\n",
      "blocks.11.ln1.hook_normalized\n",
      "blocks.11.attn.hook_q\n",
      "blocks.11.attn.hook_k\n",
      "blocks.11.attn.hook_v\n",
      "blocks.11.attn.hook_attn_scores\n",
      "blocks.11.attn.hook_pattern\n",
      "blocks.11.attn.hook_z\n",
      "blocks.11.hook_attn_out\n",
      "blocks.11.hook_resid_mid\n",
      "blocks.11.ln2.hook_scale\n",
      "blocks.11.ln2.hook_normalized\n",
      "blocks.11.mlp.transcoder.hook_sae_input\n",
      "blocks.11.mlp.transcoder.hook_sae_acts_pre\n",
      "blocks.11.mlp.transcoder.hook_sae_acts_post\n",
      "blocks.11.mlp.transcoder.hook_sae_recons\n",
      "blocks.11.mlp.mlp.hook_pre\n",
      "blocks.11.mlp.mlp.hook_post\n",
      "blocks.11.mlp.hook_sae_error\n",
      "blocks.11.mlp.hook_sae_output\n",
      "blocks.11.hook_mlp_out\n",
      "blocks.11.hook_resid_post\n",
      "ln_final.hook_scale\n",
      "ln_final.hook_normalized\n"
     ]
    }
   ],
   "source": [
    "for key in cache.keys():\n",
    "    print(key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
